{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/TransformerLensOrg/TransformerLens/blob/main/demos/LLaMA.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLAVA in TransformerLens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup (skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running as a Jupyter notebook - intended for development only!\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Using renderer: colab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2820576/659676451.py:20: DeprecationWarning:\n",
      "\n",
      "`magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
      "\n",
      "/tmp/ipykernel_2820576/659676451.py:21: DeprecationWarning:\n",
      "\n",
      "`magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# NBVAL_IGNORE_OUTPUT\n",
    "# Janky code to do different setup when run in a Colab notebook vs VSCode\n",
    "import os\n",
    "\n",
    "DEVELOPMENT_MODE = False\n",
    "IN_VSCODE = False\n",
    "IN_GITHUB = os.getenv(\"GITHUB_ACTIONS\") == \"true\"\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"Running as a Colab notebook\")\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"Running as a Jupyter notebook - intended for development only!\")\n",
    "    from IPython import get_ipython\n",
    "\n",
    "    ipython = get_ipython()\n",
    "    # Code to automatically update the HookedTransformer code as its edited without restarting the kernel\n",
    "    ipython.magic(\"load_ext autoreload\")\n",
    "    ipython.magic(\"autoreload 2\")\n",
    "    \n",
    "# %pip install transformers>=4.31.0 # Llama requires transformers>=4.31.0 and transformers in turn requires Python 3.8\n",
    "# %pip install sentencepiece # Llama tokenizer requires sentencepiece\n",
    "\n",
    "if IN_COLAB or IN_GITHUB:\n",
    "    %pip install torch\n",
    "    %pip install transformer_lens\n",
    "    %pip install circuitsvis\n",
    "    \n",
    "# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh\n",
    "import plotly.io as pio\n",
    "if IN_COLAB or not DEVELOPMENT_MODE:\n",
    "    pio.renderers.default = \"colab\"\n",
    "else:\n",
    "    pio.renderers.default = \"notebook_connected\"\n",
    "print(f\"Using renderer: {pio.renderers.default}\")\n",
    "\n",
    "import circuitsvis as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import stuff\n",
    "import torch\n",
    "import tqdm.auto as tqdm\n",
    "import plotly.express as px\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    LlavaNextForConditionalGeneration,\n",
    "    LlavaNextProcessor,\n",
    "    AutoModelForCausalLM,\n",
    ")\n",
    "# from transformers import ChameleonModel, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "from jaxtyping import Float\n",
    "\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('/aifs4su/yaodong/changye/TransformerLens')\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens.HookedLlava import HookedLlava\n",
    "import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "def imshow(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
    "    px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale=\"RdBu\", labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
    "\n",
    "def line(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
    "    px.line(utils.to_numpy(tensor), labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
    "\n",
    "def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n",
    "    x = utils.to_numpy(x)\n",
    "    y = utils.to_numpy(y)\n",
    "    px.scatter(y=y, x=x, labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs).show(renderer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading LLAVA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to load local chameleon model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f5372727f244c80b6b6bdb29a06e1ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# MODEL_PATH = \"/aifs4su/yaodong/projects/hantao/dev_cham/align-anything/outputs/0830_4k_sft_flux\"\n",
    "MODEL_PATH = \"llava-hf/llava-v1.6-mistral-7b-hf\"\n",
    "# MODEL_PATH = \"/aifs4su/yaodong/models/chameleon-7b-hf\"\n",
    "# MODEL_PATH=\"/aifs4su/yaodong/projects/hantao/anole/facilitating_image_generation/model/chameleon_hf_0830_4k\"\n",
    "\n",
    "processor = LlavaNextProcessor.from_pretrained(MODEL_PATH)\n",
    "vision_model = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "        MODEL_PATH, \n",
    "        torch_dtype=torch.float32, \n",
    "        low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "hf_model=vision_model.language_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model llava-hf/llava-v1.6-mistral-7b-hf into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedLlava.from_pretrained(\n",
    "    MODEL_PATH, \n",
    "    hf_model=hf_model,\n",
    "    torch_dtype=torch.float32, \n",
    "    low_cpu_mem_usage=True,\n",
    "    device=\"cuda:2\",\n",
    "    fold_ln=False,\n",
    "    center_writing_weights=False,\n",
    "    center_unembed=False,\n",
    "    tokenizer=None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block 0 is: MistralBlock(\n",
      "  (ln1): MistralRMSNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (ln2): MistralRMSNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (attn): MistralAttention(\n",
      "    (hook_k): HookPoint()\n",
      "    (hook_q): HookPoint()\n",
      "    (hook_v): HookPoint()\n",
      "    (hook_z): HookPoint()\n",
      "    (hook_attn_scores): HookPoint()\n",
      "    (hook_pattern): HookPoint()\n",
      "    (hook_result): HookPoint()\n",
      "    (hook_rot_k): HookPoint()\n",
      "    (hook_rot_q): HookPoint()\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): GatedMLP(\n",
      "    (hook_pre): HookPoint()\n",
      "    (hook_pre_linear): HookPoint()\n",
      "    (hook_post): HookPoint()\n",
      "  )\n",
      "  (hook_attn_in): HookPoint()\n",
      "  (hook_q_input): HookPoint()\n",
      "  (hook_k_input): HookPoint()\n",
      "  (hook_v_input): HookPoint()\n",
      "  (hook_mlp_in): HookPoint()\n",
      "  (hook_attn_out): HookPoint()\n",
      "  (hook_mlp_out): HookPoint()\n",
      "  (hook_resid_pre): HookPoint()\n",
      "  (hook_resid_mid): HookPoint()\n",
      "  (hook_resid_post): HookPoint()\n",
      ")\n",
      "Block 1 is: MistralBlock(\n",
      "  (ln1): MistralRMSNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (ln2): MistralRMSNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (attn): MistralAttention(\n",
      "    (hook_k): HookPoint()\n",
      "    (hook_q): HookPoint()\n",
      "    (hook_v): HookPoint()\n",
      "    (hook_z): HookPoint()\n",
      "    (hook_attn_scores): HookPoint()\n",
      "    (hook_pattern): HookPoint()\n",
      "    (hook_result): HookPoint()\n",
      "    (hook_rot_k): HookPoint()\n",
      "    (hook_rot_q): HookPoint()\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): GatedMLP(\n",
      "    (hook_pre): HookPoint()\n",
      "    (hook_pre_linear): HookPoint()\n",
      "    (hook_post): HookPoint()\n",
      "  )\n",
      "  (hook_attn_in): HookPoint()\n",
      "  (hook_q_input): HookPoint()\n",
      "  (hook_k_input): HookPoint()\n",
      "  (hook_v_input): HookPoint()\n",
      "  (hook_mlp_in): HookPoint()\n",
      "  (hook_attn_out): HookPoint()\n",
      "  (hook_mlp_out): HookPoint()\n",
      "  (hook_resid_pre): HookPoint()\n",
      "  (hook_resid_mid): HookPoint()\n",
      "  (hook_resid_post): HookPoint()\n",
      ")\n",
      "Block 2 is: MistralBlock(\n",
      "  (ln1): MistralRMSNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (ln2): MistralRMSNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (attn): MistralAttention(\n",
      "    (hook_k): HookPoint()\n",
      "    (hook_q): HookPoint()\n",
      "    (hook_v): HookPoint()\n",
      "    (hook_z): HookPoint()\n",
      "    (hook_attn_scores): HookPoint()\n",
      "    (hook_pattern): HookPoint()\n",
      "    (hook_result): HookPoint()\n",
      "    (hook_rot_k): HookPoint()\n",
      "    (hook_rot_q): HookPoint()\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): GatedMLP(\n",
      "    (hook_pre): HookPoint()\n",
      "    (hook_pre_linear): HookPoint()\n",
      "    (hook_post): HookPoint()\n",
      "  )\n",
      "  (hook_attn_in): HookPoint()\n",
      "  (hook_q_input): HookPoint()\n",
      "  (hook_k_input): HookPoint()\n",
      "  (hook_v_input): HookPoint()\n",
      "  (hook_mlp_in): HookPoint()\n",
      "  (hook_attn_out): HookPoint()\n",
      "  (hook_mlp_out): HookPoint()\n",
      "  (hook_resid_pre): HookPoint()\n",
      "  (hook_resid_mid): HookPoint()\n",
      "  (hook_resid_post): HookPoint()\n",
      ")\n",
      "Block 3 is: MistralBlock(\n",
      "  (ln1): MistralRMSNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (ln2): MistralRMSNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (attn): MistralAttention(\n",
      "    (hook_k): HookPoint()\n",
      "    (hook_q): HookPoint()\n",
      "    (hook_v): HookPoint()\n",
      "    (hook_z): HookPoint()\n",
      "    (hook_attn_scores): HookPoint()\n",
      "    (hook_pattern): HookPoint()\n",
      "    (hook_result): HookPoint()\n",
      "    (hook_rot_k): HookPoint()\n",
      "    (hook_rot_q): HookPoint()\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): GatedMLP(\n",
      "    (hook_pre): HookPoint()\n",
      "    (hook_pre_linear): HookPoint()\n",
      "    (hook_post): HookPoint()\n",
      "  )\n",
      "  (hook_attn_in): HookPoint()\n",
      "  (hook_q_input): HookPoint()\n",
      "  (hook_k_input): HookPoint()\n",
      "  (hook_v_input): HookPoint()\n",
      "  (hook_mlp_in): HookPoint()\n",
      "  (hook_attn_out): HookPoint()\n",
      "  (hook_mlp_out): HookPoint()\n",
      "  (hook_resid_pre): HookPoint()\n",
      "  (hook_resid_mid): HookPoint()\n",
      "  (hook_resid_post): HookPoint()\n",
      ")\n",
      "Block 4 is: MistralBlock(\n",
      "  (ln1): MistralRMSNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (ln2): MistralRMSNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (attn): MistralAttention(\n",
      "    (hook_k): HookPoint()\n",
      "    (hook_q): HookPoint()\n",
      "    (hook_v): HookPoint()\n",
      "    (hook_z): HookPoint()\n",
      "    (hook_attn_scores): HookPoint()\n",
      "    (hook_pattern): HookPoint()\n",
      "    (hook_result): HookPoint()\n",
      "    (hook_rot_k): HookPoint()\n",
      "    (hook_rot_q): HookPoint()\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): GatedMLP(\n",
      "    (hook_pre): HookPoint()\n",
      "    (hook_pre_linear): HookPoint()\n",
      "    (hook_post): HookPoint()\n",
      "  )\n",
      "  (hook_attn_in): HookPoint()\n",
      "  (hook_q_input): HookPoint()\n",
      "  (hook_k_input): HookPoint()\n",
      "  (hook_v_input): HookPoint()\n",
      "  (hook_mlp_in): HookPoint()\n",
      "  (hook_attn_out): HookPoint()\n",
      "  (hook_mlp_out): HookPoint()\n",
      "  (hook_resid_pre): HookPoint()\n",
      "  (hook_resid_mid): HookPoint()\n",
      "  (hook_resid_post): HookPoint()\n",
      ")\n",
      "Block 5 is: MistralBlock(\n",
      "  (ln1): MistralRMSNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (ln2): MistralRMSNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (attn): MistralAttention(\n",
      "    (hook_k): HookPoint()\n",
      "    (hook_q): HookPoint()\n",
      "    (hook_v): HookPoint()\n",
      "    (hook_z): HookPoint()\n",
      "    (hook_attn_scores): HookPoint()\n",
      "    (hook_pattern): HookPoint()\n",
      "    (hook_result): HookPoint()\n",
      "    (hook_rot_k): HookPoint()\n",
      "    (hook_rot_q): HookPoint()\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): GatedMLP(\n",
      "    (hook_pre): HookPoint()\n",
      "    (hook_pre_linear): HookPoint()\n",
      "    (hook_post): HookPoint()\n",
      "  )\n",
      "  (hook_attn_in): HookPoint()\n",
      "  (hook_q_input): HookPoint()\n",
      "  (hook_k_input): HookPoint()\n",
      "  (hook_v_input): HookPoint()\n",
      "  (hook_mlp_in): HookPoint()\n",
      "  (hook_attn_out): HookPoint()\n",
      "  (hook_mlp_out): HookPoint()\n",
      "  (hook_resid_pre): HookPoint()\n",
      "  (hook_resid_mid): HookPoint()\n",
      "  (hook_resid_post): HookPoint()\n",
      ")\n",
      "Block 6 is: MistralBlock(\n",
      "  (ln1): MistralRMSNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (ln2): MistralRMSNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (attn): MistralAttention(\n",
      "    (hook_k): HookPoint()\n",
      "    (hook_q): HookPoint()\n",
      "    (hook_v): HookPoint()\n",
      "    (hook_z): HookPoint()\n",
      "    (hook_attn_scores): HookPoint()\n",
      "    (hook_pattern): HookPoint()\n",
      "    (hook_result): HookPoint()\n",
      "    (hook_rot_k): HookPoint()\n",
      "    (hook_rot_q): HookPoint()\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): GatedMLP(\n",
      "    (hook_pre): HookPoint()\n",
      "    (hook_pre_linear): HookPoint()\n",
      "    (hook_post): HookPoint()\n",
      "  )\n",
      "  (hook_attn_in): HookPoint()\n",
      "  (hook_q_input): HookPoint()\n",
      "  (hook_k_input): HookPoint()\n",
      "  (hook_v_input): HookPoint()\n",
      "  (hook_mlp_in): HookPoint()\n",
      "  (hook_attn_out): HookPoint()\n",
      "  (hook_mlp_out): HookPoint()\n",
      "  (hook_resid_pre): HookPoint()\n",
      "  (hook_resid_mid): HookPoint()\n",
      "  (hook_resid_post): HookPoint()\n",
      ")\n",
      "Block 7 is: MistralBlock(\n",
      "  (ln1): MistralRMSNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (ln2): MistralRMSNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (attn): MistralAttention(\n",
      "    (hook_k): HookPoint()\n",
      "    (hook_q): HookPoint()\n",
      "    (hook_v): HookPoint()\n",
      "    (hook_z): HookPoint()\n",
      "    (hook_attn_scores): HookPoint()\n",
      "    (hook_pattern): HookPoint()\n",
      "    (hook_result): HookPoint()\n",
      "    (hook_rot_k): HookPoint()\n",
      "    (hook_rot_q): HookPoint()\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): GatedMLP(\n",
      "    (hook_pre): HookPoint()\n",
      "    (hook_pre_linear): HookPoint()\n",
      "    (hook_post): HookPoint()\n",
      "  )\n",
      "  (hook_attn_in): HookPoint()\n",
      "  (hook_q_input): HookPoint()\n",
      "  (hook_k_input): HookPoint()\n",
      "  (hook_v_input): HookPoint()\n",
      "  (hook_mlp_in): HookPoint()\n",
      "  (hook_attn_out): HookPoint()\n",
      "  (hook_mlp_out): HookPoint()\n",
      "  (hook_resid_pre): HookPoint()\n",
      "  (hook_resid_mid): HookPoint()\n",
      "  (hook_resid_post): HookPoint()\n",
      ")\n",
      "Block 8 is: MistralBlock(\n",
      "  (ln1): MistralRMSNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (ln2): MistralRMSNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (attn): MistralAttention(\n",
      "    (hook_k): HookPoint()\n",
      "    (hook_q): HookPoint()\n",
      "    (hook_v): HookPoint()\n",
      "    (hook_z): HookPoint()\n",
      "    (hook_attn_scores): HookPoint()\n",
      "    (hook_pattern): HookPoint()\n",
      "    (hook_result): HookPoint()\n",
      "    (hook_rot_k): HookPoint()\n",
      "    (hook_rot_q): HookPoint()\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): GatedMLP(\n",
      "    (hook_pre): HookPoint()\n",
      "    (hook_pre_linear): HookPoint()\n",
      "    (hook_post): HookPoint()\n",
      "  )\n",
      "  (hook_attn_in): HookPoint()\n",
      "  (hook_q_input): HookPoint()\n",
      "  (hook_k_input): HookPoint()\n",
      "  (hook_v_input): HookPoint()\n",
      "  (hook_mlp_in): HookPoint()\n",
      "  (hook_attn_out): HookPoint()\n",
      "  (hook_mlp_out): HookPoint()\n",
      "  (hook_resid_pre): HookPoint()\n",
      "  (hook_resid_mid): HookPoint()\n",
      "  (hook_resid_post): HookPoint()\n",
      ")\n",
      "Block 9 is: MistralBlock(\n",
      "  (ln1): MistralRMSNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (ln2): MistralRMSNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (attn): MistralAttention(\n",
      "    (hook_k): HookPoint()\n",
      "    (hook_q): HookPoint()\n",
      "    (hook_v): HookPoint()\n",
      "    (hook_z): HookPoint()\n",
      "    (hook_attn_scores): HookPoint()\n",
      "    (hook_pattern): HookPoint()\n",
      "    (hook_result): HookPoint()\n",
      "    (hook_rot_k): HookPoint()\n",
      "    (hook_rot_q): HookPoint()\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): GatedMLP(\n",
      "    (hook_pre): HookPoint()\n",
      "    (hook_pre_linear): HookPoint()\n",
      "    (hook_post): HookPoint()\n",
      "  )\n",
      "  (hook_attn_in): HookPoint()\n",
      "  (hook_q_input): HookPoint()\n",
      "  (hook_k_input): HookPoint()\n",
      "  (hook_v_input): HookPoint()\n",
      "  (hook_mlp_in): HookPoint()\n",
      "  (hook_attn_out): HookPoint()\n",
      "  (hook_mlp_out): HookPoint()\n",
      "  (hook_resid_pre): HookPoint()\n",
      "  (hook_resid_mid): HookPoint()\n",
      "  (hook_resid_post): HookPoint()\n",
      ")\n",
      "Block 10 is: MistralBlock(\n",
      "  (ln1): MistralRMSNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (ln2): MistralRMSNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (attn): MistralAttention(\n",
      "    (hook_k): HookPoint()\n",
      "    (hook_q): HookPoint()\n",
      "    (hook_v): HookPoint()\n",
      "    (hook_z): HookPoint()\n",
      "    (hook_attn_scores): HookPoint()\n",
      "    (hook_pattern): HookPoint()\n",
      "    (hook_result): HookPoint()\n",
      "    (hook_rot_k): HookPoint()\n",
      "    (hook_rot_q): HookPoint()\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): GatedMLP(\n",
      "    (hook_pre): HookPoint()\n",
      "    (hook_pre_linear): HookPoint()\n",
      "    (hook_post): HookPoint()\n",
      "  )\n",
      "  (hook_attn_in): HookPoint()\n",
      "  (hook_q_input): HookPoint()\n",
      "  (hook_k_input): HookPoint()\n",
      "  (hook_v_input): HookPoint()\n",
      "  (hook_mlp_in): HookPoint()\n",
      "  (hook_attn_out): HookPoint()\n",
      "  (hook_mlp_out): HookPoint()\n",
      "  (hook_resid_pre): HookPoint()\n",
      "  (hook_resid_mid): HookPoint()\n",
      "  (hook_resid_post): HookPoint()\n",
      ")\n",
      "Block 11 is: MistralBlock(\n",
      "  (ln1): MistralRMSNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (ln2): MistralRMSNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (attn): MistralAttention(\n",
      "    (hook_k): HookPoint()\n",
      "    (hook_q): HookPoint()\n",
      "    (hook_v): HookPoint()\n",
      "    (hook_z): HookPoint()\n",
      "    (hook_attn_scores): HookPoint()\n",
      "    (hook_pattern): HookPoint()\n",
      "    (hook_result): HookPoint()\n",
      "    (hook_rot_k): HookPoint()\n",
      "    (hook_rot_q): HookPoint()\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): GatedMLP(\n",
      "    (hook_pre): HookPoint()\n",
      "    (hook_pre_linear): HookPoint()\n",
      "    (hook_post): HookPoint()\n",
      "  )\n",
      "  (hook_attn_in): HookPoint()\n",
      "  (hook_q_input): HookPoint()\n",
      "  (hook_k_input): HookPoint()\n",
      "  (hook_v_input): HookPoint()\n",
      "  (hook_mlp_in): HookPoint()\n",
      "  (hook_attn_out): HookPoint()\n",
      "  (hook_mlp_out): HookPoint()\n",
      "  (hook_resid_pre): HookPoint()\n",
      "  (hook_resid_mid): HookPoint()\n",
      "  (hook_resid_post): HookPoint()\n",
      ")\n",
      "Block 12 is: MistralBlock(\n",
      "  (ln1): MistralRMSNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (ln2): MistralRMSNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (attn): MistralAttention(\n",
      "    (hook_k): HookPoint()\n",
      "    (hook_q): HookPoint()\n",
      "    (hook_v): HookPoint()\n",
      "    (hook_z): HookPoint()\n",
      "    (hook_attn_scores): HookPoint()\n",
      "    (hook_pattern): HookPoint()\n",
      "    (hook_result): HookPoint()\n",
      "    (hook_rot_k): HookPoint()\n",
      "    (hook_rot_q): HookPoint()\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): GatedMLP(\n",
      "    (hook_pre): HookPoint()\n",
      "    (hook_pre_linear): HookPoint()\n",
      "    (hook_post): HookPoint()\n",
      "  )\n",
      "  (hook_attn_in): HookPoint()\n",
      "  (hook_q_input): HookPoint()\n",
      "  (hook_k_input): HookPoint()\n",
      "  (hook_v_input): HookPoint()\n",
      "  (hook_mlp_in): HookPoint()\n",
      "  (hook_attn_out): HookPoint()\n",
      "  (hook_mlp_out): HookPoint()\n",
      "  (hook_resid_pre): HookPoint()\n",
      "  (hook_resid_mid): HookPoint()\n",
      "  (hook_resid_post): HookPoint()\n",
      ")\n",
      "Block 13 is: MistralBlock(\n",
      "  (ln1): MistralRMSNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (ln2): MistralRMSNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (attn): MistralAttention(\n",
      "    (hook_k): HookPoint()\n",
      "    (hook_q): HookPoint()\n",
      "    (hook_v): HookPoint()\n",
      "    (hook_z): HookPoint()\n",
      "    (hook_attn_scores): HookPoint()\n",
      "    (hook_pattern): HookPoint()\n",
      "    (hook_result): HookPoint()\n",
      "    (hook_rot_k): HookPoint()\n",
      "    (hook_rot_q): HookPoint()\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): GatedMLP(\n",
      "    (hook_pre): HookPoint()\n",
      "    (hook_pre_linear): HookPoint()\n",
      "    (hook_post): HookPoint()\n",
      "  )\n",
      "  (hook_attn_in): HookPoint()\n",
      "  (hook_q_input): HookPoint()\n",
      "  (hook_k_input): HookPoint()\n",
      "  (hook_v_input): HookPoint()\n",
      "  (hook_mlp_in): HookPoint()\n",
      "  (hook_attn_out): HookPoint()\n",
      "  (hook_mlp_out): HookPoint()\n",
      "  (hook_resid_pre): HookPoint()\n",
      "  (hook_resid_mid): HookPoint()\n",
      "  (hook_resid_post): HookPoint()\n",
      ")\n",
      "Block 14 is: MistralBlock(\n",
      "  (ln1): MistralRMSNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (ln2): MistralRMSNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (attn): MistralAttention(\n",
      "    (hook_k): HookPoint()\n",
      "    (hook_q): HookPoint()\n",
      "    (hook_v): HookPoint()\n",
      "    (hook_z): HookPoint()\n",
      "    (hook_attn_scores): HookPoint()\n",
      "    (hook_pattern): HookPoint()\n",
      "    (hook_result): HookPoint()\n",
      "    (hook_rot_k): HookPoint()\n",
      "    (hook_rot_q): HookPoint()\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): GatedMLP(\n",
      "    (hook_pre): HookPoint()\n",
      "    (hook_pre_linear): HookPoint()\n",
      "    (hook_post): HookPoint()\n",
      "  )\n",
      "  (hook_attn_in): HookPoint()\n",
      "  (hook_q_input): HookPoint()\n",
      "  (hook_k_input): HookPoint()\n",
      "  (hook_v_input): HookPoint()\n",
      "  (hook_mlp_in): HookPoint()\n",
      "  (hook_attn_out): HookPoint()\n",
      "  (hook_mlp_out): HookPoint()\n",
      "  (hook_resid_pre): HookPoint()\n",
      "  (hook_resid_mid): HookPoint()\n",
      "  (hook_resid_post): HookPoint()\n",
      ")\n",
      "Block 15 is: MistralBlock(\n",
      "  (ln1): MistralRMSNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (ln2): MistralRMSNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (attn): MistralAttention(\n",
      "    (hook_k): HookPoint()\n",
      "    (hook_q): HookPoint()\n",
      "    (hook_v): HookPoint()\n",
      "    (hook_z): HookPoint()\n",
      "    (hook_attn_scores): HookPoint()\n",
      "    (hook_pattern): HookPoint()\n",
      "    (hook_result): HookPoint()\n",
      "    (hook_rot_k): HookPoint()\n",
      "    (hook_rot_q): HookPoint()\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): GatedMLP(\n",
      "    (hook_pre): HookPoint()\n",
      "    (hook_pre_linear): HookPoint()\n",
      "    (hook_post): HookPoint()\n",
      "  )\n",
      "  (hook_attn_in): HookPoint()\n",
      "  (hook_q_input): HookPoint()\n",
      "  (hook_k_input): HookPoint()\n",
      "  (hook_v_input): HookPoint()\n",
      "  (hook_mlp_in): HookPoint()\n",
      "  (hook_attn_out): HookPoint()\n",
      "  (hook_mlp_out): HookPoint()\n",
      "  (hook_resid_pre): HookPoint()\n",
      "  (hook_resid_mid): HookPoint()\n",
      "  (hook_resid_post): HookPoint()\n",
      ")\n",
      "Block 16 is: MistralBlock(\n",
      "  (ln1): MistralRMSNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (ln2): MistralRMSNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (attn): MistralAttention(\n",
      "    (hook_k): HookPoint()\n",
      "    (hook_q): HookPoint()\n",
      "    (hook_v): HookPoint()\n",
      "    (hook_z): HookPoint()\n",
      "    (hook_attn_scores): HookPoint()\n",
      "    (hook_pattern): HookPoint()\n",
      "    (hook_result): HookPoint()\n",
      "    (hook_rot_k): HookPoint()\n",
      "    (hook_rot_q): HookPoint()\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): GatedMLP(\n",
      "    (hook_pre): HookPoint()\n",
      "    (hook_pre_linear): HookPoint()\n",
      "    (hook_post): HookPoint()\n",
      "  )\n",
      "  (hook_attn_in): HookPoint()\n",
      "  (hook_q_input): HookPoint()\n",
      "  (hook_k_input): HookPoint()\n",
      "  (hook_v_input): HookPoint()\n",
      "  (hook_mlp_in): HookPoint()\n",
      "  (hook_attn_out): HookPoint()\n",
      "  (hook_mlp_out): HookPoint()\n",
      "  (hook_resid_pre): HookPoint()\n",
      "  (hook_resid_mid): HookPoint()\n",
      "  (hook_resid_post): HookPoint()\n",
      ")\n",
      "Block 17 is: MistralBlock(\n",
      "  (ln1): MistralRMSNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (ln2): MistralRMSNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (attn): MistralAttention(\n",
      "    (hook_k): HookPoint()\n",
      "    (hook_q): HookPoint()\n",
      "    (hook_v): HookPoint()\n",
      "    (hook_z): HookPoint()\n",
      "    (hook_attn_scores): HookPoint()\n",
      "    (hook_pattern): HookPoint()\n",
      "    (hook_result): HookPoint()\n",
      "    (hook_rot_k): HookPoint()\n",
      "    (hook_rot_q): HookPoint()\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): GatedMLP(\n",
      "    (hook_pre): HookPoint()\n",
      "    (hook_pre_linear): HookPoint()\n",
      "    (hook_post): HookPoint()\n",
      "  )\n",
      "  (hook_attn_in): HookPoint()\n",
      "  (hook_q_input): HookPoint()\n",
      "  (hook_k_input): HookPoint()\n",
      "  (hook_v_input): HookPoint()\n",
      "  (hook_mlp_in): HookPoint()\n",
      "  (hook_attn_out): HookPoint()\n",
      "  (hook_mlp_out): HookPoint()\n",
      "  (hook_resid_pre): HookPoint()\n",
      "  (hook_resid_mid): HookPoint()\n",
      "  (hook_resid_post): HookPoint()\n",
      ")\n",
      "Block 18 is: MistralBlock(\n",
      "  (ln1): MistralRMSNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (ln2): MistralRMSNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (attn): MistralAttention(\n",
      "    (hook_k): HookPoint()\n",
      "    (hook_q): HookPoint()\n",
      "    (hook_v): HookPoint()\n",
      "    (hook_z): HookPoint()\n",
      "    (hook_attn_scores): HookPoint()\n",
      "    (hook_pattern): HookPoint()\n",
      "    (hook_result): HookPoint()\n",
      "    (hook_rot_k): HookPoint()\n",
      "    (hook_rot_q): HookPoint()\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): GatedMLP(\n",
      "    (hook_pre): HookPoint()\n",
      "    (hook_pre_linear): HookPoint()\n",
      "    (hook_post): HookPoint()\n",
      "  )\n",
      "  (hook_attn_in): HookPoint()\n",
      "  (hook_q_input): HookPoint()\n",
      "  (hook_k_input): HookPoint()\n",
      "  (hook_v_input): HookPoint()\n",
      "  (hook_mlp_in): HookPoint()\n",
      "  (hook_attn_out): HookPoint()\n",
      "  (hook_mlp_out): HookPoint()\n",
      "  (hook_resid_pre): HookPoint()\n",
      "  (hook_resid_mid): HookPoint()\n",
      "  (hook_resid_post): HookPoint()\n",
      ")\n",
      "Block 19 is: MistralBlock(\n",
      "  (ln1): MistralRMSNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (ln2): MistralRMSNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (attn): MistralAttention(\n",
      "    (hook_k): HookPoint()\n",
      "    (hook_q): HookPoint()\n",
      "    (hook_v): HookPoint()\n",
      "    (hook_z): HookPoint()\n",
      "    (hook_attn_scores): HookPoint()\n",
      "    (hook_pattern): HookPoint()\n",
      "    (hook_result): HookPoint()\n",
      "    (hook_rot_k): HookPoint()\n",
      "    (hook_rot_q): HookPoint()\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): GatedMLP(\n",
      "    (hook_pre): HookPoint()\n",
      "    (hook_pre_linear): HookPoint()\n",
      "    (hook_post): HookPoint()\n",
      "  )\n",
      "  (hook_attn_in): HookPoint()\n",
      "  (hook_q_input): HookPoint()\n",
      "  (hook_k_input): HookPoint()\n",
      "  (hook_v_input): HookPoint()\n",
      "  (hook_mlp_in): HookPoint()\n",
      "  (hook_attn_out): HookPoint()\n",
      "  (hook_mlp_out): HookPoint()\n",
      "  (hook_resid_pre): HookPoint()\n",
      "  (hook_resid_mid): HookPoint()\n",
      "  (hook_resid_post): HookPoint()\n",
      ")\n",
      "Block 20 is: MistralBlock(\n",
      "  (ln1): MistralRMSNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (ln2): MistralRMSNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (attn): MistralAttention(\n",
      "    (hook_k): HookPoint()\n",
      "    (hook_q): HookPoint()\n",
      "    (hook_v): HookPoint()\n",
      "    (hook_z): HookPoint()\n",
      "    (hook_attn_scores): HookPoint()\n",
      "    (hook_pattern): HookPoint()\n",
      "    (hook_result): HookPoint()\n",
      "    (hook_rot_k): HookPoint()\n",
      "    (hook_rot_q): HookPoint()\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): GatedMLP(\n",
      "    (hook_pre): HookPoint()\n",
      "    (hook_pre_linear): HookPoint()\n",
      "    (hook_post): HookPoint()\n",
      "  )\n",
      "  (hook_attn_in): HookPoint()\n",
      "  (hook_q_input): HookPoint()\n",
      "  (hook_k_input): HookPoint()\n",
      "  (hook_v_input): HookPoint()\n",
      "  (hook_mlp_in): HookPoint()\n",
      "  (hook_attn_out): HookPoint()\n",
      "  (hook_mlp_out): HookPoint()\n",
      "  (hook_resid_pre): HookPoint()\n",
      "  (hook_resid_mid): HookPoint()\n",
      "  (hook_resid_post): HookPoint()\n",
      ")\n",
      "Block 21 is: MistralBlock(\n",
      "  (ln1): MistralRMSNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (ln2): MistralRMSNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (attn): MistralAttention(\n",
      "    (hook_k): HookPoint()\n",
      "    (hook_q): HookPoint()\n",
      "    (hook_v): HookPoint()\n",
      "    (hook_z): HookPoint()\n",
      "    (hook_attn_scores): HookPoint()\n",
      "    (hook_pattern): HookPoint()\n",
      "    (hook_result): HookPoint()\n",
      "    (hook_rot_k): HookPoint()\n",
      "    (hook_rot_q): HookPoint()\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): GatedMLP(\n",
      "    (hook_pre): HookPoint()\n",
      "    (hook_pre_linear): HookPoint()\n",
      "    (hook_post): HookPoint()\n",
      "  )\n",
      "  (hook_attn_in): HookPoint()\n",
      "  (hook_q_input): HookPoint()\n",
      "  (hook_k_input): HookPoint()\n",
      "  (hook_v_input): HookPoint()\n",
      "  (hook_mlp_in): HookPoint()\n",
      "  (hook_attn_out): HookPoint()\n",
      "  (hook_mlp_out): HookPoint()\n",
      "  (hook_resid_pre): HookPoint()\n",
      "  (hook_resid_mid): HookPoint()\n",
      "  (hook_resid_post): HookPoint()\n",
      ")\n",
      "Block 22 is: MistralBlock(\n",
      "  (ln1): MistralRMSNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (ln2): MistralRMSNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (attn): MistralAttention(\n",
      "    (hook_k): HookPoint()\n",
      "    (hook_q): HookPoint()\n",
      "    (hook_v): HookPoint()\n",
      "    (hook_z): HookPoint()\n",
      "    (hook_attn_scores): HookPoint()\n",
      "    (hook_pattern): HookPoint()\n",
      "    (hook_result): HookPoint()\n",
      "    (hook_rot_k): HookPoint()\n",
      "    (hook_rot_q): HookPoint()\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): GatedMLP(\n",
      "    (hook_pre): HookPoint()\n",
      "    (hook_pre_linear): HookPoint()\n",
      "    (hook_post): HookPoint()\n",
      "  )\n",
      "  (hook_attn_in): HookPoint()\n",
      "  (hook_q_input): HookPoint()\n",
      "  (hook_k_input): HookPoint()\n",
      "  (hook_v_input): HookPoint()\n",
      "  (hook_mlp_in): HookPoint()\n",
      "  (hook_attn_out): HookPoint()\n",
      "  (hook_mlp_out): HookPoint()\n",
      "  (hook_resid_pre): HookPoint()\n",
      "  (hook_resid_mid): HookPoint()\n",
      "  (hook_resid_post): HookPoint()\n",
      ")\n",
      "Block 23 is: MistralBlock(\n",
      "  (ln1): MistralRMSNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (ln2): MistralRMSNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (attn): MistralAttention(\n",
      "    (hook_k): HookPoint()\n",
      "    (hook_q): HookPoint()\n",
      "    (hook_v): HookPoint()\n",
      "    (hook_z): HookPoint()\n",
      "    (hook_attn_scores): HookPoint()\n",
      "    (hook_pattern): HookPoint()\n",
      "    (hook_result): HookPoint()\n",
      "    (hook_rot_k): HookPoint()\n",
      "    (hook_rot_q): HookPoint()\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): GatedMLP(\n",
      "    (hook_pre): HookPoint()\n",
      "    (hook_pre_linear): HookPoint()\n",
      "    (hook_post): HookPoint()\n",
      "  )\n",
      "  (hook_attn_in): HookPoint()\n",
      "  (hook_q_input): HookPoint()\n",
      "  (hook_k_input): HookPoint()\n",
      "  (hook_v_input): HookPoint()\n",
      "  (hook_mlp_in): HookPoint()\n",
      "  (hook_attn_out): HookPoint()\n",
      "  (hook_mlp_out): HookPoint()\n",
      "  (hook_resid_pre): HookPoint()\n",
      "  (hook_resid_mid): HookPoint()\n",
      "  (hook_resid_post): HookPoint()\n",
      ")\n",
      "Block 24 is: MistralBlock(\n",
      "  (ln1): MistralRMSNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (ln2): MistralRMSNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (attn): MistralAttention(\n",
      "    (hook_k): HookPoint()\n",
      "    (hook_q): HookPoint()\n",
      "    (hook_v): HookPoint()\n",
      "    (hook_z): HookPoint()\n",
      "    (hook_attn_scores): HookPoint()\n",
      "    (hook_pattern): HookPoint()\n",
      "    (hook_result): HookPoint()\n",
      "    (hook_rot_k): HookPoint()\n",
      "    (hook_rot_q): HookPoint()\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): GatedMLP(\n",
      "    (hook_pre): HookPoint()\n",
      "    (hook_pre_linear): HookPoint()\n",
      "    (hook_post): HookPoint()\n",
      "  )\n",
      "  (hook_attn_in): HookPoint()\n",
      "  (hook_q_input): HookPoint()\n",
      "  (hook_k_input): HookPoint()\n",
      "  (hook_v_input): HookPoint()\n",
      "  (hook_mlp_in): HookPoint()\n",
      "  (hook_attn_out): HookPoint()\n",
      "  (hook_mlp_out): HookPoint()\n",
      "  (hook_resid_pre): HookPoint()\n",
      "  (hook_resid_mid): HookPoint()\n",
      "  (hook_resid_post): HookPoint()\n",
      ")\n",
      "Block 25 is: MistralBlock(\n",
      "  (ln1): MistralRMSNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (ln2): MistralRMSNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (attn): MistralAttention(\n",
      "    (hook_k): HookPoint()\n",
      "    (hook_q): HookPoint()\n",
      "    (hook_v): HookPoint()\n",
      "    (hook_z): HookPoint()\n",
      "    (hook_attn_scores): HookPoint()\n",
      "    (hook_pattern): HookPoint()\n",
      "    (hook_result): HookPoint()\n",
      "    (hook_rot_k): HookPoint()\n",
      "    (hook_rot_q): HookPoint()\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): GatedMLP(\n",
      "    (hook_pre): HookPoint()\n",
      "    (hook_pre_linear): HookPoint()\n",
      "    (hook_post): HookPoint()\n",
      "  )\n",
      "  (hook_attn_in): HookPoint()\n",
      "  (hook_q_input): HookPoint()\n",
      "  (hook_k_input): HookPoint()\n",
      "  (hook_v_input): HookPoint()\n",
      "  (hook_mlp_in): HookPoint()\n",
      "  (hook_attn_out): HookPoint()\n",
      "  (hook_mlp_out): HookPoint()\n",
      "  (hook_resid_pre): HookPoint()\n",
      "  (hook_resid_mid): HookPoint()\n",
      "  (hook_resid_post): HookPoint()\n",
      ")\n",
      "Block 26 is: MistralBlock(\n",
      "  (ln1): MistralRMSNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (ln2): MistralRMSNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (attn): MistralAttention(\n",
      "    (hook_k): HookPoint()\n",
      "    (hook_q): HookPoint()\n",
      "    (hook_v): HookPoint()\n",
      "    (hook_z): HookPoint()\n",
      "    (hook_attn_scores): HookPoint()\n",
      "    (hook_pattern): HookPoint()\n",
      "    (hook_result): HookPoint()\n",
      "    (hook_rot_k): HookPoint()\n",
      "    (hook_rot_q): HookPoint()\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): GatedMLP(\n",
      "    (hook_pre): HookPoint()\n",
      "    (hook_pre_linear): HookPoint()\n",
      "    (hook_post): HookPoint()\n",
      "  )\n",
      "  (hook_attn_in): HookPoint()\n",
      "  (hook_q_input): HookPoint()\n",
      "  (hook_k_input): HookPoint()\n",
      "  (hook_v_input): HookPoint()\n",
      "  (hook_mlp_in): HookPoint()\n",
      "  (hook_attn_out): HookPoint()\n",
      "  (hook_mlp_out): HookPoint()\n",
      "  (hook_resid_pre): HookPoint()\n",
      "  (hook_resid_mid): HookPoint()\n",
      "  (hook_resid_post): HookPoint()\n",
      ")\n",
      "Block 27 is: MistralBlock(\n",
      "  (ln1): MistralRMSNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (ln2): MistralRMSNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (attn): MistralAttention(\n",
      "    (hook_k): HookPoint()\n",
      "    (hook_q): HookPoint()\n",
      "    (hook_v): HookPoint()\n",
      "    (hook_z): HookPoint()\n",
      "    (hook_attn_scores): HookPoint()\n",
      "    (hook_pattern): HookPoint()\n",
      "    (hook_result): HookPoint()\n",
      "    (hook_rot_k): HookPoint()\n",
      "    (hook_rot_q): HookPoint()\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): GatedMLP(\n",
      "    (hook_pre): HookPoint()\n",
      "    (hook_pre_linear): HookPoint()\n",
      "    (hook_post): HookPoint()\n",
      "  )\n",
      "  (hook_attn_in): HookPoint()\n",
      "  (hook_q_input): HookPoint()\n",
      "  (hook_k_input): HookPoint()\n",
      "  (hook_v_input): HookPoint()\n",
      "  (hook_mlp_in): HookPoint()\n",
      "  (hook_attn_out): HookPoint()\n",
      "  (hook_mlp_out): HookPoint()\n",
      "  (hook_resid_pre): HookPoint()\n",
      "  (hook_resid_mid): HookPoint()\n",
      "  (hook_resid_post): HookPoint()\n",
      ")\n",
      "Block 28 is: MistralBlock(\n",
      "  (ln1): MistralRMSNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (ln2): MistralRMSNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (attn): MistralAttention(\n",
      "    (hook_k): HookPoint()\n",
      "    (hook_q): HookPoint()\n",
      "    (hook_v): HookPoint()\n",
      "    (hook_z): HookPoint()\n",
      "    (hook_attn_scores): HookPoint()\n",
      "    (hook_pattern): HookPoint()\n",
      "    (hook_result): HookPoint()\n",
      "    (hook_rot_k): HookPoint()\n",
      "    (hook_rot_q): HookPoint()\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): GatedMLP(\n",
      "    (hook_pre): HookPoint()\n",
      "    (hook_pre_linear): HookPoint()\n",
      "    (hook_post): HookPoint()\n",
      "  )\n",
      "  (hook_attn_in): HookPoint()\n",
      "  (hook_q_input): HookPoint()\n",
      "  (hook_k_input): HookPoint()\n",
      "  (hook_v_input): HookPoint()\n",
      "  (hook_mlp_in): HookPoint()\n",
      "  (hook_attn_out): HookPoint()\n",
      "  (hook_mlp_out): HookPoint()\n",
      "  (hook_resid_pre): HookPoint()\n",
      "  (hook_resid_mid): HookPoint()\n",
      "  (hook_resid_post): HookPoint()\n",
      ")\n",
      "Block 29 is: MistralBlock(\n",
      "  (ln1): MistralRMSNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (ln2): MistralRMSNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (attn): MistralAttention(\n",
      "    (hook_k): HookPoint()\n",
      "    (hook_q): HookPoint()\n",
      "    (hook_v): HookPoint()\n",
      "    (hook_z): HookPoint()\n",
      "    (hook_attn_scores): HookPoint()\n",
      "    (hook_pattern): HookPoint()\n",
      "    (hook_result): HookPoint()\n",
      "    (hook_rot_k): HookPoint()\n",
      "    (hook_rot_q): HookPoint()\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): GatedMLP(\n",
      "    (hook_pre): HookPoint()\n",
      "    (hook_pre_linear): HookPoint()\n",
      "    (hook_post): HookPoint()\n",
      "  )\n",
      "  (hook_attn_in): HookPoint()\n",
      "  (hook_q_input): HookPoint()\n",
      "  (hook_k_input): HookPoint()\n",
      "  (hook_v_input): HookPoint()\n",
      "  (hook_mlp_in): HookPoint()\n",
      "  (hook_attn_out): HookPoint()\n",
      "  (hook_mlp_out): HookPoint()\n",
      "  (hook_resid_pre): HookPoint()\n",
      "  (hook_resid_mid): HookPoint()\n",
      "  (hook_resid_post): HookPoint()\n",
      ")\n",
      "Block 30 is: MistralBlock(\n",
      "  (ln1): MistralRMSNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (ln2): MistralRMSNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (attn): MistralAttention(\n",
      "    (hook_k): HookPoint()\n",
      "    (hook_q): HookPoint()\n",
      "    (hook_v): HookPoint()\n",
      "    (hook_z): HookPoint()\n",
      "    (hook_attn_scores): HookPoint()\n",
      "    (hook_pattern): HookPoint()\n",
      "    (hook_result): HookPoint()\n",
      "    (hook_rot_k): HookPoint()\n",
      "    (hook_rot_q): HookPoint()\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): GatedMLP(\n",
      "    (hook_pre): HookPoint()\n",
      "    (hook_pre_linear): HookPoint()\n",
      "    (hook_post): HookPoint()\n",
      "  )\n",
      "  (hook_attn_in): HookPoint()\n",
      "  (hook_q_input): HookPoint()\n",
      "  (hook_k_input): HookPoint()\n",
      "  (hook_v_input): HookPoint()\n",
      "  (hook_mlp_in): HookPoint()\n",
      "  (hook_attn_out): HookPoint()\n",
      "  (hook_mlp_out): HookPoint()\n",
      "  (hook_resid_pre): HookPoint()\n",
      "  (hook_resid_mid): HookPoint()\n",
      "  (hook_resid_post): HookPoint()\n",
      ")\n",
      "Block 31 is: MistralBlock(\n",
      "  (ln1): MistralRMSNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (ln2): MistralRMSNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (attn): MistralAttention(\n",
      "    (hook_k): HookPoint()\n",
      "    (hook_q): HookPoint()\n",
      "    (hook_v): HookPoint()\n",
      "    (hook_z): HookPoint()\n",
      "    (hook_attn_scores): HookPoint()\n",
      "    (hook_pattern): HookPoint()\n",
      "    (hook_result): HookPoint()\n",
      "    (hook_rot_k): HookPoint()\n",
      "    (hook_rot_q): HookPoint()\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): GatedMLP(\n",
      "    (hook_pre): HookPoint()\n",
      "    (hook_pre_linear): HookPoint()\n",
      "    (hook_post): HookPoint()\n",
      "  )\n",
      "  (hook_attn_in): HookPoint()\n",
      "  (hook_q_input): HookPoint()\n",
      "  (hook_k_input): HookPoint()\n",
      "  (hook_v_input): HookPoint()\n",
      "  (hook_mlp_in): HookPoint()\n",
      "  (hook_attn_out): HookPoint()\n",
      "  (hook_mlp_out): HookPoint()\n",
      "  (hook_resid_pre): HookPoint()\n",
      "  (hook_resid_mid): HookPoint()\n",
      "  (hook_resid_post): HookPoint()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "blocks_and_idxs = list(zip(range(model.cfg.n_layers), model.blocks))\n",
    "for i, block in blocks_and_idxs:\n",
    "    print(f\"Block {i} is: {block}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block 0 is: MistralDecoderLayer(\n",
      "  (self_attn): MistralAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): MistralMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "Block 1 is: MistralDecoderLayer(\n",
      "  (self_attn): MistralAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): MistralMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "Block 2 is: MistralDecoderLayer(\n",
      "  (self_attn): MistralAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): MistralMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "Block 3 is: MistralDecoderLayer(\n",
      "  (self_attn): MistralAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): MistralMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "Block 4 is: MistralDecoderLayer(\n",
      "  (self_attn): MistralAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): MistralMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "Block 5 is: MistralDecoderLayer(\n",
      "  (self_attn): MistralAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): MistralMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "Block 6 is: MistralDecoderLayer(\n",
      "  (self_attn): MistralAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): MistralMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "Block 7 is: MistralDecoderLayer(\n",
      "  (self_attn): MistralAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): MistralMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "Block 8 is: MistralDecoderLayer(\n",
      "  (self_attn): MistralAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): MistralMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "Block 9 is: MistralDecoderLayer(\n",
      "  (self_attn): MistralAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): MistralMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "Block 10 is: MistralDecoderLayer(\n",
      "  (self_attn): MistralAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): MistralMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "Block 11 is: MistralDecoderLayer(\n",
      "  (self_attn): MistralAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): MistralMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "Block 12 is: MistralDecoderLayer(\n",
      "  (self_attn): MistralAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): MistralMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "Block 13 is: MistralDecoderLayer(\n",
      "  (self_attn): MistralAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): MistralMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "Block 14 is: MistralDecoderLayer(\n",
      "  (self_attn): MistralAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): MistralMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "Block 15 is: MistralDecoderLayer(\n",
      "  (self_attn): MistralAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): MistralMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "Block 16 is: MistralDecoderLayer(\n",
      "  (self_attn): MistralAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): MistralMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "Block 17 is: MistralDecoderLayer(\n",
      "  (self_attn): MistralAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): MistralMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "Block 18 is: MistralDecoderLayer(\n",
      "  (self_attn): MistralAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): MistralMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "Block 19 is: MistralDecoderLayer(\n",
      "  (self_attn): MistralAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): MistralMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "Block 20 is: MistralDecoderLayer(\n",
      "  (self_attn): MistralAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): MistralMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "Block 21 is: MistralDecoderLayer(\n",
      "  (self_attn): MistralAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): MistralMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "Block 22 is: MistralDecoderLayer(\n",
      "  (self_attn): MistralAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): MistralMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "Block 23 is: MistralDecoderLayer(\n",
      "  (self_attn): MistralAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): MistralMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "Block 24 is: MistralDecoderLayer(\n",
      "  (self_attn): MistralAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): MistralMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "Block 25 is: MistralDecoderLayer(\n",
      "  (self_attn): MistralAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): MistralMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "Block 26 is: MistralDecoderLayer(\n",
      "  (self_attn): MistralAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): MistralMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "Block 27 is: MistralDecoderLayer(\n",
      "  (self_attn): MistralAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): MistralMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "Block 28 is: MistralDecoderLayer(\n",
      "  (self_attn): MistralAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): MistralMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "Block 29 is: MistralDecoderLayer(\n",
      "  (self_attn): MistralAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): MistralMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "Block 30 is: MistralDecoderLayer(\n",
      "  (self_attn): MistralAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): MistralMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "Block 31 is: MistralDecoderLayer(\n",
      "  (self_attn): MistralAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): MistralMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "hf_blocks_and_idxs = list(zip(range(hf_model.config.num_hidden_layers), hf_model.model.layers))\n",
    "\n",
    "for i, block in hf_blocks_and_idxs:\n",
    "    print(f\"Block {i} is: {block}\")\n",
    "# print(hf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['embed.W_E', 'blocks.0.ln1.w', 'blocks.0.ln2.w', 'blocks.0.attn.W_Q', 'blocks.0.attn.W_O', 'blocks.0.attn.b_Q', 'blocks.0.attn.b_O', 'blocks.0.attn.W_K', 'blocks.0.attn.W_V', 'blocks.0.attn.b_K', 'blocks.0.attn.b_V', 'blocks.0.attn.mask', 'blocks.0.attn.IGNORE', 'blocks.0.attn.rotary_sin', 'blocks.0.attn.rotary_cos', 'blocks.0.attn.o_proj.weight', 'blocks.0.mlp.W_in', 'blocks.0.mlp.W_out', 'blocks.0.mlp.W_gate', 'blocks.0.mlp.b_in', 'blocks.0.mlp.b_out', 'blocks.1.ln1.w', 'blocks.1.ln2.w', 'blocks.1.attn.W_Q', 'blocks.1.attn.W_O', 'blocks.1.attn.b_Q', 'blocks.1.attn.b_O', 'blocks.1.attn.W_K', 'blocks.1.attn.W_V', 'blocks.1.attn.b_K', 'blocks.1.attn.b_V', 'blocks.1.attn.mask', 'blocks.1.attn.IGNORE', 'blocks.1.attn.rotary_sin', 'blocks.1.attn.rotary_cos', 'blocks.1.attn.o_proj.weight', 'blocks.1.mlp.W_in', 'blocks.1.mlp.W_out', 'blocks.1.mlp.W_gate', 'blocks.1.mlp.b_in', 'blocks.1.mlp.b_out', 'blocks.2.ln1.w', 'blocks.2.ln2.w', 'blocks.2.attn.W_Q', 'blocks.2.attn.W_O', 'blocks.2.attn.b_Q', 'blocks.2.attn.b_O', 'blocks.2.attn.W_K', 'blocks.2.attn.W_V', 'blocks.2.attn.b_K', 'blocks.2.attn.b_V', 'blocks.2.attn.mask', 'blocks.2.attn.IGNORE', 'blocks.2.attn.rotary_sin', 'blocks.2.attn.rotary_cos', 'blocks.2.attn.o_proj.weight', 'blocks.2.mlp.W_in', 'blocks.2.mlp.W_out', 'blocks.2.mlp.W_gate', 'blocks.2.mlp.b_in', 'blocks.2.mlp.b_out', 'blocks.3.ln1.w', 'blocks.3.ln2.w', 'blocks.3.attn.W_Q', 'blocks.3.attn.W_O', 'blocks.3.attn.b_Q', 'blocks.3.attn.b_O', 'blocks.3.attn.W_K', 'blocks.3.attn.W_V', 'blocks.3.attn.b_K', 'blocks.3.attn.b_V', 'blocks.3.attn.mask', 'blocks.3.attn.IGNORE', 'blocks.3.attn.rotary_sin', 'blocks.3.attn.rotary_cos', 'blocks.3.attn.o_proj.weight', 'blocks.3.mlp.W_in', 'blocks.3.mlp.W_out', 'blocks.3.mlp.W_gate', 'blocks.3.mlp.b_in', 'blocks.3.mlp.b_out', 'blocks.4.ln1.w', 'blocks.4.ln2.w', 'blocks.4.attn.W_Q', 'blocks.4.attn.W_O', 'blocks.4.attn.b_Q', 'blocks.4.attn.b_O', 'blocks.4.attn.W_K', 'blocks.4.attn.W_V', 'blocks.4.attn.b_K', 'blocks.4.attn.b_V', 'blocks.4.attn.mask', 'blocks.4.attn.IGNORE', 'blocks.4.attn.rotary_sin', 'blocks.4.attn.rotary_cos', 'blocks.4.attn.o_proj.weight', 'blocks.4.mlp.W_in', 'blocks.4.mlp.W_out', 'blocks.4.mlp.W_gate', 'blocks.4.mlp.b_in', 'blocks.4.mlp.b_out', 'blocks.5.ln1.w', 'blocks.5.ln2.w', 'blocks.5.attn.W_Q', 'blocks.5.attn.W_O', 'blocks.5.attn.b_Q', 'blocks.5.attn.b_O', 'blocks.5.attn.W_K', 'blocks.5.attn.W_V', 'blocks.5.attn.b_K', 'blocks.5.attn.b_V', 'blocks.5.attn.mask', 'blocks.5.attn.IGNORE', 'blocks.5.attn.rotary_sin', 'blocks.5.attn.rotary_cos', 'blocks.5.attn.o_proj.weight', 'blocks.5.mlp.W_in', 'blocks.5.mlp.W_out', 'blocks.5.mlp.W_gate', 'blocks.5.mlp.b_in', 'blocks.5.mlp.b_out', 'blocks.6.ln1.w', 'blocks.6.ln2.w', 'blocks.6.attn.W_Q', 'blocks.6.attn.W_O', 'blocks.6.attn.b_Q', 'blocks.6.attn.b_O', 'blocks.6.attn.W_K', 'blocks.6.attn.W_V', 'blocks.6.attn.b_K', 'blocks.6.attn.b_V', 'blocks.6.attn.mask', 'blocks.6.attn.IGNORE', 'blocks.6.attn.rotary_sin', 'blocks.6.attn.rotary_cos', 'blocks.6.attn.o_proj.weight', 'blocks.6.mlp.W_in', 'blocks.6.mlp.W_out', 'blocks.6.mlp.W_gate', 'blocks.6.mlp.b_in', 'blocks.6.mlp.b_out', 'blocks.7.ln1.w', 'blocks.7.ln2.w', 'blocks.7.attn.W_Q', 'blocks.7.attn.W_O', 'blocks.7.attn.b_Q', 'blocks.7.attn.b_O', 'blocks.7.attn.W_K', 'blocks.7.attn.W_V', 'blocks.7.attn.b_K', 'blocks.7.attn.b_V', 'blocks.7.attn.mask', 'blocks.7.attn.IGNORE', 'blocks.7.attn.rotary_sin', 'blocks.7.attn.rotary_cos', 'blocks.7.attn.o_proj.weight', 'blocks.7.mlp.W_in', 'blocks.7.mlp.W_out', 'blocks.7.mlp.W_gate', 'blocks.7.mlp.b_in', 'blocks.7.mlp.b_out', 'blocks.8.ln1.w', 'blocks.8.ln2.w', 'blocks.8.attn.W_Q', 'blocks.8.attn.W_O', 'blocks.8.attn.b_Q', 'blocks.8.attn.b_O', 'blocks.8.attn.W_K', 'blocks.8.attn.W_V', 'blocks.8.attn.b_K', 'blocks.8.attn.b_V', 'blocks.8.attn.mask', 'blocks.8.attn.IGNORE', 'blocks.8.attn.rotary_sin', 'blocks.8.attn.rotary_cos', 'blocks.8.attn.o_proj.weight', 'blocks.8.mlp.W_in', 'blocks.8.mlp.W_out', 'blocks.8.mlp.W_gate', 'blocks.8.mlp.b_in', 'blocks.8.mlp.b_out', 'blocks.9.ln1.w', 'blocks.9.ln2.w', 'blocks.9.attn.W_Q', 'blocks.9.attn.W_O', 'blocks.9.attn.b_Q', 'blocks.9.attn.b_O', 'blocks.9.attn.W_K', 'blocks.9.attn.W_V', 'blocks.9.attn.b_K', 'blocks.9.attn.b_V', 'blocks.9.attn.mask', 'blocks.9.attn.IGNORE', 'blocks.9.attn.rotary_sin', 'blocks.9.attn.rotary_cos', 'blocks.9.attn.o_proj.weight', 'blocks.9.mlp.W_in', 'blocks.9.mlp.W_out', 'blocks.9.mlp.W_gate', 'blocks.9.mlp.b_in', 'blocks.9.mlp.b_out', 'blocks.10.ln1.w', 'blocks.10.ln2.w', 'blocks.10.attn.W_Q', 'blocks.10.attn.W_O', 'blocks.10.attn.b_Q', 'blocks.10.attn.b_O', 'blocks.10.attn.W_K', 'blocks.10.attn.W_V', 'blocks.10.attn.b_K', 'blocks.10.attn.b_V', 'blocks.10.attn.mask', 'blocks.10.attn.IGNORE', 'blocks.10.attn.rotary_sin', 'blocks.10.attn.rotary_cos', 'blocks.10.attn.o_proj.weight', 'blocks.10.mlp.W_in', 'blocks.10.mlp.W_out', 'blocks.10.mlp.W_gate', 'blocks.10.mlp.b_in', 'blocks.10.mlp.b_out', 'blocks.11.ln1.w', 'blocks.11.ln2.w', 'blocks.11.attn.W_Q', 'blocks.11.attn.W_O', 'blocks.11.attn.b_Q', 'blocks.11.attn.b_O', 'blocks.11.attn.W_K', 'blocks.11.attn.W_V', 'blocks.11.attn.b_K', 'blocks.11.attn.b_V', 'blocks.11.attn.mask', 'blocks.11.attn.IGNORE', 'blocks.11.attn.rotary_sin', 'blocks.11.attn.rotary_cos', 'blocks.11.attn.o_proj.weight', 'blocks.11.mlp.W_in', 'blocks.11.mlp.W_out', 'blocks.11.mlp.W_gate', 'blocks.11.mlp.b_in', 'blocks.11.mlp.b_out', 'blocks.12.ln1.w', 'blocks.12.ln2.w', 'blocks.12.attn.W_Q', 'blocks.12.attn.W_O', 'blocks.12.attn.b_Q', 'blocks.12.attn.b_O', 'blocks.12.attn.W_K', 'blocks.12.attn.W_V', 'blocks.12.attn.b_K', 'blocks.12.attn.b_V', 'blocks.12.attn.mask', 'blocks.12.attn.IGNORE', 'blocks.12.attn.rotary_sin', 'blocks.12.attn.rotary_cos', 'blocks.12.attn.o_proj.weight', 'blocks.12.mlp.W_in', 'blocks.12.mlp.W_out', 'blocks.12.mlp.W_gate', 'blocks.12.mlp.b_in', 'blocks.12.mlp.b_out', 'blocks.13.ln1.w', 'blocks.13.ln2.w', 'blocks.13.attn.W_Q', 'blocks.13.attn.W_O', 'blocks.13.attn.b_Q', 'blocks.13.attn.b_O', 'blocks.13.attn.W_K', 'blocks.13.attn.W_V', 'blocks.13.attn.b_K', 'blocks.13.attn.b_V', 'blocks.13.attn.mask', 'blocks.13.attn.IGNORE', 'blocks.13.attn.rotary_sin', 'blocks.13.attn.rotary_cos', 'blocks.13.attn.o_proj.weight', 'blocks.13.mlp.W_in', 'blocks.13.mlp.W_out', 'blocks.13.mlp.W_gate', 'blocks.13.mlp.b_in', 'blocks.13.mlp.b_out', 'blocks.14.ln1.w', 'blocks.14.ln2.w', 'blocks.14.attn.W_Q', 'blocks.14.attn.W_O', 'blocks.14.attn.b_Q', 'blocks.14.attn.b_O', 'blocks.14.attn.W_K', 'blocks.14.attn.W_V', 'blocks.14.attn.b_K', 'blocks.14.attn.b_V', 'blocks.14.attn.mask', 'blocks.14.attn.IGNORE', 'blocks.14.attn.rotary_sin', 'blocks.14.attn.rotary_cos', 'blocks.14.attn.o_proj.weight', 'blocks.14.mlp.W_in', 'blocks.14.mlp.W_out', 'blocks.14.mlp.W_gate', 'blocks.14.mlp.b_in', 'blocks.14.mlp.b_out', 'blocks.15.ln1.w', 'blocks.15.ln2.w', 'blocks.15.attn.W_Q', 'blocks.15.attn.W_O', 'blocks.15.attn.b_Q', 'blocks.15.attn.b_O', 'blocks.15.attn.W_K', 'blocks.15.attn.W_V', 'blocks.15.attn.b_K', 'blocks.15.attn.b_V', 'blocks.15.attn.mask', 'blocks.15.attn.IGNORE', 'blocks.15.attn.rotary_sin', 'blocks.15.attn.rotary_cos', 'blocks.15.attn.o_proj.weight', 'blocks.15.mlp.W_in', 'blocks.15.mlp.W_out', 'blocks.15.mlp.W_gate', 'blocks.15.mlp.b_in', 'blocks.15.mlp.b_out', 'blocks.16.ln1.w', 'blocks.16.ln2.w', 'blocks.16.attn.W_Q', 'blocks.16.attn.W_O', 'blocks.16.attn.b_Q', 'blocks.16.attn.b_O', 'blocks.16.attn.W_K', 'blocks.16.attn.W_V', 'blocks.16.attn.b_K', 'blocks.16.attn.b_V', 'blocks.16.attn.mask', 'blocks.16.attn.IGNORE', 'blocks.16.attn.rotary_sin', 'blocks.16.attn.rotary_cos', 'blocks.16.attn.o_proj.weight', 'blocks.16.mlp.W_in', 'blocks.16.mlp.W_out', 'blocks.16.mlp.W_gate', 'blocks.16.mlp.b_in', 'blocks.16.mlp.b_out', 'blocks.17.ln1.w', 'blocks.17.ln2.w', 'blocks.17.attn.W_Q', 'blocks.17.attn.W_O', 'blocks.17.attn.b_Q', 'blocks.17.attn.b_O', 'blocks.17.attn.W_K', 'blocks.17.attn.W_V', 'blocks.17.attn.b_K', 'blocks.17.attn.b_V', 'blocks.17.attn.mask', 'blocks.17.attn.IGNORE', 'blocks.17.attn.rotary_sin', 'blocks.17.attn.rotary_cos', 'blocks.17.attn.o_proj.weight', 'blocks.17.mlp.W_in', 'blocks.17.mlp.W_out', 'blocks.17.mlp.W_gate', 'blocks.17.mlp.b_in', 'blocks.17.mlp.b_out', 'blocks.18.ln1.w', 'blocks.18.ln2.w', 'blocks.18.attn.W_Q', 'blocks.18.attn.W_O', 'blocks.18.attn.b_Q', 'blocks.18.attn.b_O', 'blocks.18.attn.W_K', 'blocks.18.attn.W_V', 'blocks.18.attn.b_K', 'blocks.18.attn.b_V', 'blocks.18.attn.mask', 'blocks.18.attn.IGNORE', 'blocks.18.attn.rotary_sin', 'blocks.18.attn.rotary_cos', 'blocks.18.attn.o_proj.weight', 'blocks.18.mlp.W_in', 'blocks.18.mlp.W_out', 'blocks.18.mlp.W_gate', 'blocks.18.mlp.b_in', 'blocks.18.mlp.b_out', 'blocks.19.ln1.w', 'blocks.19.ln2.w', 'blocks.19.attn.W_Q', 'blocks.19.attn.W_O', 'blocks.19.attn.b_Q', 'blocks.19.attn.b_O', 'blocks.19.attn.W_K', 'blocks.19.attn.W_V', 'blocks.19.attn.b_K', 'blocks.19.attn.b_V', 'blocks.19.attn.mask', 'blocks.19.attn.IGNORE', 'blocks.19.attn.rotary_sin', 'blocks.19.attn.rotary_cos', 'blocks.19.attn.o_proj.weight', 'blocks.19.mlp.W_in', 'blocks.19.mlp.W_out', 'blocks.19.mlp.W_gate', 'blocks.19.mlp.b_in', 'blocks.19.mlp.b_out', 'blocks.20.ln1.w', 'blocks.20.ln2.w', 'blocks.20.attn.W_Q', 'blocks.20.attn.W_O', 'blocks.20.attn.b_Q', 'blocks.20.attn.b_O', 'blocks.20.attn.W_K', 'blocks.20.attn.W_V', 'blocks.20.attn.b_K', 'blocks.20.attn.b_V', 'blocks.20.attn.mask', 'blocks.20.attn.IGNORE', 'blocks.20.attn.rotary_sin', 'blocks.20.attn.rotary_cos', 'blocks.20.attn.o_proj.weight', 'blocks.20.mlp.W_in', 'blocks.20.mlp.W_out', 'blocks.20.mlp.W_gate', 'blocks.20.mlp.b_in', 'blocks.20.mlp.b_out', 'blocks.21.ln1.w', 'blocks.21.ln2.w', 'blocks.21.attn.W_Q', 'blocks.21.attn.W_O', 'blocks.21.attn.b_Q', 'blocks.21.attn.b_O', 'blocks.21.attn.W_K', 'blocks.21.attn.W_V', 'blocks.21.attn.b_K', 'blocks.21.attn.b_V', 'blocks.21.attn.mask', 'blocks.21.attn.IGNORE', 'blocks.21.attn.rotary_sin', 'blocks.21.attn.rotary_cos', 'blocks.21.attn.o_proj.weight', 'blocks.21.mlp.W_in', 'blocks.21.mlp.W_out', 'blocks.21.mlp.W_gate', 'blocks.21.mlp.b_in', 'blocks.21.mlp.b_out', 'blocks.22.ln1.w', 'blocks.22.ln2.w', 'blocks.22.attn.W_Q', 'blocks.22.attn.W_O', 'blocks.22.attn.b_Q', 'blocks.22.attn.b_O', 'blocks.22.attn.W_K', 'blocks.22.attn.W_V', 'blocks.22.attn.b_K', 'blocks.22.attn.b_V', 'blocks.22.attn.mask', 'blocks.22.attn.IGNORE', 'blocks.22.attn.rotary_sin', 'blocks.22.attn.rotary_cos', 'blocks.22.attn.o_proj.weight', 'blocks.22.mlp.W_in', 'blocks.22.mlp.W_out', 'blocks.22.mlp.W_gate', 'blocks.22.mlp.b_in', 'blocks.22.mlp.b_out', 'blocks.23.ln1.w', 'blocks.23.ln2.w', 'blocks.23.attn.W_Q', 'blocks.23.attn.W_O', 'blocks.23.attn.b_Q', 'blocks.23.attn.b_O', 'blocks.23.attn.W_K', 'blocks.23.attn.W_V', 'blocks.23.attn.b_K', 'blocks.23.attn.b_V', 'blocks.23.attn.mask', 'blocks.23.attn.IGNORE', 'blocks.23.attn.rotary_sin', 'blocks.23.attn.rotary_cos', 'blocks.23.attn.o_proj.weight', 'blocks.23.mlp.W_in', 'blocks.23.mlp.W_out', 'blocks.23.mlp.W_gate', 'blocks.23.mlp.b_in', 'blocks.23.mlp.b_out', 'blocks.24.ln1.w', 'blocks.24.ln2.w', 'blocks.24.attn.W_Q', 'blocks.24.attn.W_O', 'blocks.24.attn.b_Q', 'blocks.24.attn.b_O', 'blocks.24.attn.W_K', 'blocks.24.attn.W_V', 'blocks.24.attn.b_K', 'blocks.24.attn.b_V', 'blocks.24.attn.mask', 'blocks.24.attn.IGNORE', 'blocks.24.attn.rotary_sin', 'blocks.24.attn.rotary_cos', 'blocks.24.attn.o_proj.weight', 'blocks.24.mlp.W_in', 'blocks.24.mlp.W_out', 'blocks.24.mlp.W_gate', 'blocks.24.mlp.b_in', 'blocks.24.mlp.b_out', 'blocks.25.ln1.w', 'blocks.25.ln2.w', 'blocks.25.attn.W_Q', 'blocks.25.attn.W_O', 'blocks.25.attn.b_Q', 'blocks.25.attn.b_O', 'blocks.25.attn.W_K', 'blocks.25.attn.W_V', 'blocks.25.attn.b_K', 'blocks.25.attn.b_V', 'blocks.25.attn.mask', 'blocks.25.attn.IGNORE', 'blocks.25.attn.rotary_sin', 'blocks.25.attn.rotary_cos', 'blocks.25.attn.o_proj.weight', 'blocks.25.mlp.W_in', 'blocks.25.mlp.W_out', 'blocks.25.mlp.W_gate', 'blocks.25.mlp.b_in', 'blocks.25.mlp.b_out', 'blocks.26.ln1.w', 'blocks.26.ln2.w', 'blocks.26.attn.W_Q', 'blocks.26.attn.W_O', 'blocks.26.attn.b_Q', 'blocks.26.attn.b_O', 'blocks.26.attn.W_K', 'blocks.26.attn.W_V', 'blocks.26.attn.b_K', 'blocks.26.attn.b_V', 'blocks.26.attn.mask', 'blocks.26.attn.IGNORE', 'blocks.26.attn.rotary_sin', 'blocks.26.attn.rotary_cos', 'blocks.26.attn.o_proj.weight', 'blocks.26.mlp.W_in', 'blocks.26.mlp.W_out', 'blocks.26.mlp.W_gate', 'blocks.26.mlp.b_in', 'blocks.26.mlp.b_out', 'blocks.27.ln1.w', 'blocks.27.ln2.w', 'blocks.27.attn.W_Q', 'blocks.27.attn.W_O', 'blocks.27.attn.b_Q', 'blocks.27.attn.b_O', 'blocks.27.attn.W_K', 'blocks.27.attn.W_V', 'blocks.27.attn.b_K', 'blocks.27.attn.b_V', 'blocks.27.attn.mask', 'blocks.27.attn.IGNORE', 'blocks.27.attn.rotary_sin', 'blocks.27.attn.rotary_cos', 'blocks.27.attn.o_proj.weight', 'blocks.27.mlp.W_in', 'blocks.27.mlp.W_out', 'blocks.27.mlp.W_gate', 'blocks.27.mlp.b_in', 'blocks.27.mlp.b_out', 'blocks.28.ln1.w', 'blocks.28.ln2.w', 'blocks.28.attn.W_Q', 'blocks.28.attn.W_O', 'blocks.28.attn.b_Q', 'blocks.28.attn.b_O', 'blocks.28.attn.W_K', 'blocks.28.attn.W_V', 'blocks.28.attn.b_K', 'blocks.28.attn.b_V', 'blocks.28.attn.mask', 'blocks.28.attn.IGNORE', 'blocks.28.attn.rotary_sin', 'blocks.28.attn.rotary_cos', 'blocks.28.attn.o_proj.weight', 'blocks.28.mlp.W_in', 'blocks.28.mlp.W_out', 'blocks.28.mlp.W_gate', 'blocks.28.mlp.b_in', 'blocks.28.mlp.b_out', 'blocks.29.ln1.w', 'blocks.29.ln2.w', 'blocks.29.attn.W_Q', 'blocks.29.attn.W_O', 'blocks.29.attn.b_Q', 'blocks.29.attn.b_O', 'blocks.29.attn.W_K', 'blocks.29.attn.W_V', 'blocks.29.attn.b_K', 'blocks.29.attn.b_V', 'blocks.29.attn.mask', 'blocks.29.attn.IGNORE', 'blocks.29.attn.rotary_sin', 'blocks.29.attn.rotary_cos', 'blocks.29.attn.o_proj.weight', 'blocks.29.mlp.W_in', 'blocks.29.mlp.W_out', 'blocks.29.mlp.W_gate', 'blocks.29.mlp.b_in', 'blocks.29.mlp.b_out', 'blocks.30.ln1.w', 'blocks.30.ln2.w', 'blocks.30.attn.W_Q', 'blocks.30.attn.W_O', 'blocks.30.attn.b_Q', 'blocks.30.attn.b_O', 'blocks.30.attn.W_K', 'blocks.30.attn.W_V', 'blocks.30.attn.b_K', 'blocks.30.attn.b_V', 'blocks.30.attn.mask', 'blocks.30.attn.IGNORE', 'blocks.30.attn.rotary_sin', 'blocks.30.attn.rotary_cos', 'blocks.30.attn.o_proj.weight', 'blocks.30.mlp.W_in', 'blocks.30.mlp.W_out', 'blocks.30.mlp.W_gate', 'blocks.30.mlp.b_in', 'blocks.30.mlp.b_out', 'blocks.31.ln1.w', 'blocks.31.ln2.w', 'blocks.31.attn.W_Q', 'blocks.31.attn.W_O', 'blocks.31.attn.b_Q', 'blocks.31.attn.b_O', 'blocks.31.attn.W_K', 'blocks.31.attn.W_V', 'blocks.31.attn.b_K', 'blocks.31.attn.b_V', 'blocks.31.attn.mask', 'blocks.31.attn.IGNORE', 'blocks.31.attn.rotary_sin', 'blocks.31.attn.rotary_cos', 'blocks.31.attn.o_proj.weight', 'blocks.31.mlp.W_in', 'blocks.31.mlp.W_out', 'blocks.31.mlp.W_gate', 'blocks.31.mlp.b_in', 'blocks.31.mlp.b_out', 'ln_final.w', 'unembed.W_U', 'unembed.b_U'])\n",
      "odict_keys(['model.embed_tokens.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.0.mlp.down_proj.weight', 'model.layers.0.input_layernorm.weight', 'model.layers.0.post_attention_layernorm.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.1.input_layernorm.weight', 'model.layers.1.post_attention_layernorm.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.2.input_layernorm.weight', 'model.layers.2.post_attention_layernorm.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.3.input_layernorm.weight', 'model.layers.3.post_attention_layernorm.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.4.input_layernorm.weight', 'model.layers.4.post_attention_layernorm.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.5.input_layernorm.weight', 'model.layers.5.post_attention_layernorm.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.6.input_layernorm.weight', 'model.layers.6.post_attention_layernorm.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.7.input_layernorm.weight', 'model.layers.7.post_attention_layernorm.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.8.input_layernorm.weight', 'model.layers.8.post_attention_layernorm.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.v_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.9.input_layernorm.weight', 'model.layers.9.post_attention_layernorm.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.10.input_layernorm.weight', 'model.layers.10.post_attention_layernorm.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.11.input_layernorm.weight', 'model.layers.11.post_attention_layernorm.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.12.input_layernorm.weight', 'model.layers.12.post_attention_layernorm.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.13.input_layernorm.weight', 'model.layers.13.post_attention_layernorm.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.14.input_layernorm.weight', 'model.layers.14.post_attention_layernorm.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.15.input_layernorm.weight', 'model.layers.15.post_attention_layernorm.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.16.input_layernorm.weight', 'model.layers.16.post_attention_layernorm.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.17.input_layernorm.weight', 'model.layers.17.post_attention_layernorm.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.18.input_layernorm.weight', 'model.layers.18.post_attention_layernorm.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.19.input_layernorm.weight', 'model.layers.19.post_attention_layernorm.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.20.input_layernorm.weight', 'model.layers.20.post_attention_layernorm.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.21.input_layernorm.weight', 'model.layers.21.post_attention_layernorm.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.22.input_layernorm.weight', 'model.layers.22.post_attention_layernorm.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.23.input_layernorm.weight', 'model.layers.23.post_attention_layernorm.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.24.input_layernorm.weight', 'model.layers.24.post_attention_layernorm.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.25.input_layernorm.weight', 'model.layers.25.post_attention_layernorm.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.26.input_layernorm.weight', 'model.layers.26.post_attention_layernorm.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.27.input_layernorm.weight', 'model.layers.27.post_attention_layernorm.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.28.input_layernorm.weight', 'model.layers.28.post_attention_layernorm.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.29.input_layernorm.weight', 'model.layers.29.post_attention_layernorm.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.30.input_layernorm.weight', 'model.layers.30.post_attention_layernorm.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.31.input_layernorm.weight', 'model.layers.31.post_attention_layernorm.weight', 'model.norm.weight', 'lm_head.weight'])\n"
     ]
    }
   ],
   "source": [
    "block_params = model.state_dict()\n",
    "hf_block_params = hf_model.state_dict()\n",
    "print(block_params.keys())\n",
    "print(hf_block_params.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for (i, block), (j, hf_block) in zip(blocks_and_idxs, hf_blocks_and_idxs):\n",
    "#     assert block == hf_block, f\"Block {i} does not match: {block} vs {hf_block}\"\n",
    "# print(model.blocks[0].attn.query.weight.shape)\n",
    "import einops\n",
    "for i in range(model.cfg.n_layers):\n",
    "    W_Q=einops.rearrange(block_params[f\"blocks.{i}.attn.W_Q\"], \"n m h -> (n h) m\")\n",
    "    W_K=einops.rearrange(block_params[f\"blocks.{i}.attn.W_K\"], \"n m h -> (n h) m\")\n",
    "    W_V=einops.rearrange(block_params[f\"blocks.{i}.attn.W_V\"], \"n m h -> (n h) m\")\n",
    "    W_O=einops.rearrange(block_params[f\"blocks.{i}.attn.W_O\"], \"n h m -> m (n h)\")\n",
    "    \n",
    "    device = \"cuda:2\"\n",
    "    if not torch.equal(W_Q.to(device),hf_block_params[f\"model.layers.{i}.self_attn.q_proj.weight\"].to(device)):\n",
    "        print(f\"Block {i} W_Q does not match\")\n",
    "    if not torch.equal(W_K.to(device),hf_block_params[f\"model.layers.{i}.self_attn.k_proj.weight\"].to(device)):\n",
    "        print(f\"Block {i} W_K does not match\")\n",
    "    if not torch.equal(W_V.to(device),hf_block_params[f\"model.layers.{i}.self_attn.v_proj.weight\"].to(device)):\n",
    "        print(f\"Block {i} W_V does not match\")\n",
    "    if not torch.equal(W_O.to(device),hf_block_params[f\"model.layers.{i}.self_attn.o_proj.weight\"].to(device)):\n",
    "        print(f\"Block {i} W_O does not match\")\n",
    "    # print(torch.equal(W_Q.to(device),hf_block_params[f\"model.layers.{i}.self_attn.q_proj.weight\"].to(device)))\n",
    "# W_Q=einops.rearrange(block_params[f\"blocks.{0}.attn.W_Q\"], \"n m h -> (n h) m\")\n",
    "# print(W_Q.shape)\n",
    "# device = \"cuda:2\"\n",
    "# print(torch.equal(W_Q.to(device),hf_block_params[f\"model.layers.{0}.self_attn.q_proj.weight\"].to(device)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model.blocks[0].attn.norm_Q.weight)\n",
    "# print(\"=\"*10)\n",
    "# print(model.blocks[0].attn.norm_Q.bias)\n",
    "# print(\"=\"*10)\n",
    "# print(model.blocks[0].attn.norm_K.weight)\n",
    "# print(\"=\"*10)\n",
    "# print(model.blocks[0].attn.norm_K.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,  6926,   349,   272,  5565,   302,  7293, 28804]])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da8bfdf03dd94fd192242e273b394dfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where is the capital of Germany? making a love a or a or a or a or or or or or or or or Jie\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Where is the capital of Germany?\"\n",
    "input = processor(prompt, return_tensors=\"pt\")\n",
    "input_ids = input.input_ids\n",
    "print(input_ids)\n",
    "output = model.generate(input_ids, max_new_tokens=20, temperature=0)\n",
    "print(processor.tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MistralBlock(\n",
      "  (ln1): MistralRMSNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (ln2): MistralRMSNorm(\n",
      "    (hook_scale): HookPoint()\n",
      "    (hook_normalized): HookPoint()\n",
      "  )\n",
      "  (attn): MistralAttention(\n",
      "    (hook_k): HookPoint()\n",
      "    (hook_q): HookPoint()\n",
      "    (hook_v): HookPoint()\n",
      "    (hook_z): HookPoint()\n",
      "    (hook_attn_scores): HookPoint()\n",
      "    (hook_pattern): HookPoint()\n",
      "    (hook_result): HookPoint()\n",
      "    (hook_rot_k): HookPoint()\n",
      "    (hook_rot_q): HookPoint()\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): GatedMLP(\n",
      "    (hook_pre): HookPoint()\n",
      "    (hook_pre_linear): HookPoint()\n",
      "    (hook_post): HookPoint()\n",
      "  )\n",
      "  (hook_attn_in): HookPoint()\n",
      "  (hook_q_input): HookPoint()\n",
      "  (hook_k_input): HookPoint()\n",
      "  (hook_v_input): HookPoint()\n",
      "  (hook_mlp_in): HookPoint()\n",
      "  (hook_attn_out): HookPoint()\n",
      "  (hook_mlp_out): HookPoint()\n",
      "  (hook_resid_pre): HookPoint()\n",
      "  (hook_resid_mid): HookPoint()\n",
      "  (hook_resid_post): HookPoint()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model.blocks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "        \"The capital of Germany is\",\n",
    "        \"2 * 42 = \", \n",
    "        \"My favorite\", \n",
    "        \"aosetuhaosuh aostud aoestuaoentsudhasuh aos tasat naostutshaosuhtnaoe usaho uaotsnhuaosntuhaosntu haouaoshat u saotheu saonuh aoesntuhaosut aosu thaosu thaoustaho usaothusaothuao sutao sutaotduaoetudet uaosthuao uaostuaoeu aostouhsaonh aosnthuaoscnuhaoshkbaoesnit haosuhaoe uasotehusntaosn.p.uo ksoentudhao ustahoeuaso usant.hsa otuhaotsi aostuhs\",\n",
    "    ]\n",
    "    \n",
    "    # \n",
    "model.eval()\n",
    "hf_model.eval()\n",
    "tokenizer=AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "# print(tokenizer)\n",
    "#  GPU \n",
    "model_device =\"cuda:0\"\n",
    "hf_model_device = \"cuda:1\"\n",
    "model=model.to(model_device)\n",
    "hf_model=hf_model.to(hf_model_device)\n",
    "    \n",
    "    #  prompt\n",
    "#  prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1:  prompt  tokenizer\n",
    "prompt = \"What is the capital of France?\"\n",
    "\n",
    "#  Hugging Face  tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "#  HookedTransformer  Hugging Face \n",
    "prompt_id_tl = tokenizer.encode(prompt, return_tensors=\"pt\").to(model_device)  # HookedTransformer \n",
    "prompt_id_hf = tokenizer.encode(prompt, return_tensors=\"pt\").to(hf_model_device)  # Hugging Face \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: \n",
    "\n",
    "# \n",
    "def hook_fn(module_name, module, input, output):\n",
    "    if isinstance(output, tuple):\n",
    "        output = output[0]  # \n",
    "    return {module_name: output.detach().cpu()}\n",
    "\n",
    "#  HookedTransformer  Hugging Face \n",
    "tl_internal_outputs = {}\n",
    "hf_internal_outputs = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: \n",
    "\n",
    "#  Hugging Face input_layernorm, self_attn, mlp\n",
    "def register_hf_hooks(hf_model):\n",
    "    hf_model.model.layers[0].input_layernorm.register_forward_hook(lambda m, i, o: hf_internal_outputs.update(hook_fn(\"input_layernorm\", m, i, o)))\n",
    "    hf_model.model.layers[0].self_attn.q_proj.register_forward_hook(lambda m, i, o: hf_internal_outputs.update(hook_fn(\"self_attn.q_proj\", m, i, o)))\n",
    "    hf_model.model.layers[0].self_attn.o_proj.register_forward_hook(lambda m, i, o: hf_internal_outputs.update(hook_fn(\"self_attn.o_proj\", m, i, o)))\n",
    "    hf_model.model.layers[0].mlp.gate_proj.register_forward_hook(lambda m, i, o: hf_internal_outputs.update(hook_fn(\"mlp.gate_proj\", m, i, o)))\n",
    "    hf_model.model.layers[0].mlp.down_proj.register_forward_hook(lambda m, i, o: hf_internal_outputs.update(hook_fn(\"mlp.down_proj\", m, i, o)))\n",
    "\n",
    "#  HookedTransformer \n",
    "def register_tl_hooks(model):\n",
    "    model.blocks[0].hook_resid_pre.register_forward_hook(lambda m, i, o: tl_internal_outputs.update(hook_fn(\"hook_resid_pre\", m, i, o)))\n",
    "    model.blocks[0].attn.hook_q.register_forward_hook(lambda m, i, o: tl_internal_outputs.update(hook_fn(\"hook_attn_in\", m, i, o)))\n",
    "    model.blocks[0].attn.hook_z.register_forward_hook(lambda m, i, o: tl_internal_outputs.update(hook_fn(\"hook_attn_out\", m, i, o)))\n",
    "    model.blocks[0].mlp.hook_pre.register_forward_hook(lambda m, i, o: tl_internal_outputs.update(hook_fn(\"hook_mlp_in\", m, i, o)))\n",
    "    model.blocks[0].mlp.hook_post.register_forward_hook(lambda m, i, o: tl_internal_outputs.update(hook_fn(\"hook_mlp_out\", m, i, o)))\n",
    "\n",
    "#  Hugging Face \n",
    "register_hf_hooks(hf_model)\n",
    "\n",
    "#  HookedTransformer \n",
    "register_tl_hooks(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n",
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "print(hf_model.device)\n",
    "print(prompt_id_hf.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: \n",
    "tl_logits = model(prompt_id_tl).detach().cpu()\n",
    "hf_logits = hf_model(prompt_id_hf).logits.detach().cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 32, 128])\n",
      "torch.Size([1, 8, 4096])\n",
      "torch.Size([1, 32, 8, 128])\n",
      "torch.Size([1, 8, 4096])\n",
      "Difference found in hook_attn_out (TL) vs self_attn.o_proj (HF):\n",
      "HookedTransformer output: tensor([[[ 7.2933e-03,  5.3986e-03,  1.2297e-03,  ..., -9.5093e-04,\n",
      "          -7.8503e-04, -8.8677e-04],\n",
      "         [ 3.9709e-03, -7.0594e-03,  4.5734e-04,  ..., -1.6104e-02,\n",
      "          -2.2545e-02,  3.1311e-02],\n",
      "         [ 7.2933e-03,  5.3986e-03,  1.2297e-03,  ..., -1.3243e-03,\n",
      "           1.0687e-04, -8.0034e-04],\n",
      "         ...,\n",
      "         [ 3.9709e-03, -7.0594e-03,  4.5734e-04,  ...,  1.8842e-02,\n",
      "          -3.0881e-03,  6.8312e-03],\n",
      "         [ 7.2933e-03,  5.3986e-03,  1.2297e-03,  ..., -2.8347e-03,\n",
      "           2.3358e-03, -7.3878e-05],\n",
      "         [ 3.9709e-03, -7.0594e-03,  4.5734e-04,  ...,  1.1225e-02,\n",
      "          -8.1580e-03, -3.4510e-04]]])\n",
      "Hugging Face output: tensor([[[ 0.0019,  0.0014,  0.0015,  ...,  0.0006,  0.0006,  0.0010],\n",
      "         [-0.0017, -0.0008,  0.0002,  ...,  0.0019,  0.0004,  0.0004],\n",
      "         [-0.0043, -0.0031, -0.0004,  ...,  0.0029,  0.0008, -0.0003],\n",
      "         ...,\n",
      "         [ 0.0017, -0.0026, -0.0024,  ..., -0.0017,  0.0016,  0.0019],\n",
      "         [-0.0004, -0.0005, -0.0016,  ..., -0.0013, -0.0007, -0.0008],\n",
      "         [ 0.0013, -0.0008, -0.0002,  ...,  0.0006,  0.0001,  0.0009]]])\n",
      "Difference: tensor([[[ 0.0054,  0.0040, -0.0002,  ..., -0.0015, -0.0014, -0.0019],\n",
      "         [ 0.0057, -0.0062,  0.0003,  ..., -0.0180, -0.0229,  0.0309],\n",
      "         [ 0.0116,  0.0085,  0.0016,  ..., -0.0042, -0.0007, -0.0005],\n",
      "         ...,\n",
      "         [ 0.0023, -0.0044,  0.0028,  ...,  0.0205, -0.0047,  0.0049],\n",
      "         [ 0.0077,  0.0059,  0.0029,  ..., -0.0015,  0.0031,  0.0007],\n",
      "         [ 0.0027, -0.0063,  0.0007,  ...,  0.0106, -0.0083, -0.0012]]])\n",
      "torch.Size([1, 8, 14336])\n",
      "torch.Size([1, 8, 14336])\n",
      "Difference found in hook_mlp_in (TL) vs mlp.gate_proj (HF):\n",
      "HookedTransformer output: tensor([[[ 0.0373, -0.0288,  0.0186,  ..., -0.1207,  0.0011,  0.1007],\n",
      "         [ 0.0069, -0.1576,  0.0005,  ...,  0.0653,  0.2141,  0.1840],\n",
      "         [ 0.0177, -0.1268,  0.0835,  ...,  0.0121,  0.0963,  0.0047],\n",
      "         ...,\n",
      "         [ 0.0003,  0.0766,  0.0751,  ..., -0.1464, -0.0475,  0.0199],\n",
      "         [-0.0755,  0.1318,  0.0452,  ...,  0.0765, -0.0314, -0.1291],\n",
      "         [ 0.0300,  0.0003, -0.0663,  ...,  0.0418,  0.0098,  0.1006]]])\n",
      "Hugging Face output: tensor([[[-0.0195,  0.0075, -0.0035,  ..., -0.0280,  0.0044,  0.0119],\n",
      "         [ 0.0643, -0.0160,  0.0705,  ...,  0.0715, -0.0600, -0.0868],\n",
      "         [ 0.1534,  0.0157,  0.1454,  ...,  0.0841, -0.1512, -0.1936],\n",
      "         ...,\n",
      "         [ 0.0274,  0.0827, -0.0258,  ...,  0.0002, -0.1315, -0.0942],\n",
      "         [ 0.0060,  0.1067, -0.0677,  ...,  0.0546, -0.0263, -0.0505],\n",
      "         [ 0.0334, -0.0118, -0.1004,  ...,  0.0021,  0.0307,  0.0292]]])\n",
      "Difference: tensor([[[ 0.0568, -0.0363,  0.0221,  ..., -0.0927, -0.0033,  0.0888],\n",
      "         [-0.0575, -0.1416, -0.0701,  ..., -0.0062,  0.2741,  0.2708],\n",
      "         [-0.1357, -0.1425, -0.0618,  ..., -0.0721,  0.2475,  0.1983],\n",
      "         ...,\n",
      "         [-0.0271, -0.0061,  0.1009,  ..., -0.1466,  0.0840,  0.1141],\n",
      "         [-0.0815,  0.0251,  0.1129,  ...,  0.0219, -0.0050, -0.0786],\n",
      "         [-0.0034,  0.0121,  0.0341,  ...,  0.0397, -0.0209,  0.0715]]])\n",
      "torch.Size([1, 8, 14336])\n",
      "torch.Size([1, 8, 4096])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (14336) must match the size of tensor b (4096) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tl_key\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhook_attn_in\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m tl_key\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhook_attn_out\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     17\u001b[0m     tl_value\u001b[38;5;241m=\u001b[39mtl_value\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m4096\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mallclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtl_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhf_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43matol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-2\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDifference found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtl_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (TL) vs \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhf_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (HF):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHookedTransformer output: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtl_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (14336) must match the size of tensor b (4096) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "# Step 5: \n",
    "module_mapping = {\n",
    "    \"hook_attn_in\": \"self_attn.q_proj\",\n",
    "    \"hook_attn_out\": \"self_attn.o_proj\",\n",
    "    \"hook_mlp_in\": \"mlp.gate_proj\",\n",
    "    \"hook_mlp_out\": \"mlp.down_proj\"\n",
    "}\n",
    "\n",
    "for tl_key, hf_key in module_mapping.items():\n",
    "    if tl_key in tl_internal_outputs and hf_key in hf_internal_outputs:\n",
    "        tl_value = tl_internal_outputs[tl_key]\n",
    "        hf_value = hf_internal_outputs[hf_key]\n",
    "        print(tl_value.shape)\n",
    "        \n",
    "        print(hf_value.shape)\n",
    "        if tl_key==\"hook_attn_in\" or tl_key==\"hook_attn_out\":\n",
    "            tl_value=tl_value.reshape(1,8,4096)\n",
    "\n",
    "        if not torch.allclose(tl_value, hf_value, atol=1e-4, rtol=1e-2):\n",
    "            print(f\"Difference found in {tl_key} (TL) vs {hf_key} (HF):\")\n",
    "            print(f\"HookedTransformer output: {tl_value}\")\n",
    "            print(f\"Hugging Face output: {hf_value}\")\n",
    "            print(f\"Difference: {tl_value - hf_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "hook_fn() missing 1 required positional argument: 'output'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[155], line 51\u001b[0m\n\u001b[1;32m     49\u001b[0m prompt_id_hf \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(hf_model_device)  \u001b[38;5;66;03m# Hugging Face \u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# \u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m tl_logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_id_tl\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m     52\u001b[0m hf_logits \u001b[38;5;241m=\u001b[39m hf_model(prompt_id_hf)\u001b[38;5;241m.\u001b[39mlogits\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# \u001b[39;00m\n",
      "File \u001b[0;32m/aifs4su/yaodong/miniconda3/envs/sae/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/aifs4su/yaodong/miniconda3/envs/sae/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/aifs4su/yaodong/changye/TransformerLens/transformer_lens/HookedLlava.py:566\u001b[0m, in \u001b[0;36mHookedLlava.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, attention_mask, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    562\u001b[0m         shortformer_pos_embed \u001b[38;5;241m=\u001b[39m shortformer_pos_embed\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m    563\u001b[0m             devices\u001b[38;5;241m.\u001b[39mget_device_for_block_index(i, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg)\n\u001b[1;32m    564\u001b[0m         )\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;66;03m# import pdb; pdb.set_trace()\u001b[39;00m\n\u001b[0;32m--> 566\u001b[0m     residual \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each\u001b[39;49;00m\n\u001b[1;32m    569\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# block\u001b[39;49;00m\n\u001b[1;32m    570\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stop_at_layer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;66;03m# When we stop at an early layer, we end here rather than doing further computation\u001b[39;00m\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m residual\n",
      "File \u001b[0;32m/aifs4su/yaodong/miniconda3/envs/sae/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/aifs4su/yaodong/miniconda3/envs/sae/lib/python3.10/site-packages/torch/nn/modules/module.py:1568\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1565\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1566\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1568\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1569\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1570\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1571\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1572\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1573\u001b[0m     ):\n\u001b[1;32m   1574\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m/aifs4su/yaodong/changye/TransformerLens/transformer_lens/components/transformer_block.py:124\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry, attention_mask)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    108\u001b[0m     resid_pre: Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch pos d_model\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    111\u001b[0m     attention_mask: Optional[Int[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch offset_pos\u001b[39m\u001b[38;5;124m\"\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    112\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch pos d_model\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    113\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"A single Transformer block.\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \n\u001b[1;32m    115\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;124;03m        Float[torch.Tensor, \"batch pos d_model\"]: Our resulting tensor\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m     resid_pre \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhook_resid_pre\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresid_pre\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39muse_attn_in \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39muse_split_qkv_input:\n\u001b[1;32m    127\u001b[0m         \u001b[38;5;66;03m# We're adding a head dimension\u001b[39;00m\n\u001b[1;32m    128\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m shortformer_pos_embed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/aifs4su/yaodong/miniconda3/envs/sae/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/aifs4su/yaodong/miniconda3/envs/sae/lib/python3.10/site-packages/torch/nn/modules/module.py:1581\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1579\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, args, kwargs, result)\n\u001b[1;32m   1580\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1581\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1584\u001b[0m     result \u001b[38;5;241m=\u001b[39m hook_result\n",
      "Cell \u001b[0;32mIn[140], line 30\u001b[0m, in \u001b[0;36mregister_tl_hooks.<locals>.<lambda>\u001b[0;34m(m, i, o)\u001b[0m\n\u001b[1;32m     28\u001b[0m model\u001b[38;5;241m.\u001b[39mblocks[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mattn\u001b[38;5;241m.\u001b[39mhook_v\u001b[38;5;241m.\u001b[39mregister_forward_hook(\u001b[38;5;28;01mlambda\u001b[39;00m m, i, o: tl_internal_outputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattn.hook_v\u001b[39m\u001b[38;5;124m\"\u001b[39m: hook_fn(m, i, o)}))\n\u001b[1;32m     29\u001b[0m model\u001b[38;5;241m.\u001b[39mblocks[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mattn\u001b[38;5;241m.\u001b[39mhook_z\u001b[38;5;241m.\u001b[39mregister_forward_hook(\u001b[38;5;28;01mlambda\u001b[39;00m m, i, o: tl_internal_outputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattn.hook_z\u001b[39m\u001b[38;5;124m\"\u001b[39m: hook_fn(m, i, o)}))\n\u001b[0;32m---> 30\u001b[0m model\u001b[38;5;241m.\u001b[39mblocks[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mhook_resid_pre\u001b[38;5;241m.\u001b[39mregister_forward_hook(\u001b[38;5;28;01mlambda\u001b[39;00m m, i, o: tl_internal_outputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhook_resid_pre\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mhook_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m}))\n\u001b[1;32m     31\u001b[0m model\u001b[38;5;241m.\u001b[39mblocks[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mhook_resid_post\u001b[38;5;241m.\u001b[39mregister_forward_hook(\u001b[38;5;28;01mlambda\u001b[39;00m m, i, o: tl_internal_outputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhook_resid_post\u001b[39m\u001b[38;5;124m\"\u001b[39m: hook_fn(m, i, o)}))\n",
      "\u001b[0;31mTypeError\u001b[0m: hook_fn() missing 1 required positional argument: 'output'"
     ]
    }
   ],
   "source": [
    "# \n",
    "def hook_fn(module_name, module, input, output):\n",
    "    if isinstance(output, tuple):\n",
    "        output = output[0]  # \n",
    "    return {module_name: output.detach().cpu()}\n",
    "\n",
    "#  HookedTransformer  Hugging Face \n",
    "tl_internal_outputs = {}\n",
    "hf_internal_outputs = {}\n",
    "\n",
    "#  HookedTransformer  Hugging Face \n",
    "module_mapping = {\n",
    "    \"hook_resid_pre\": \"input_layernorm\",\n",
    "    \"hook_attn_in\": \"self_attn.q_proj\",\n",
    "    \"hook_attn_out\": \"self_attn.o_proj\",\n",
    "    \"hook_mlp_in\": \"mlp.gate_proj\",\n",
    "    \"hook_mlp_out\": \"mlp.down_proj\"\n",
    "}\n",
    "\n",
    "#  Hugging Face input_layernorm, self_attn, mlp\n",
    "def register_hf_hooks(hf_model):\n",
    "    hf_model.model.layers[0].input_layernorm.register_forward_hook(lambda m, i, o: hf_internal_outputs.update(hook_fn(\"input_layernorm\", m, i, o)))\n",
    "    hf_model.model.layers[0].self_attn.q_proj.register_forward_hook(lambda m, i, o: hf_internal_outputs.update(hook_fn(\"self_attn.q_proj\", m, i, o)))\n",
    "    hf_model.model.layers[0].self_attn.o_proj.register_forward_hook(lambda m, i, o: hf_internal_outputs.update(hook_fn(\"self_attn.o_proj\", m, i, o)))\n",
    "    hf_model.model.layers[0].mlp.gate_proj.register_forward_hook(lambda m, i, o: hf_internal_outputs.update(hook_fn(\"mlp.gate_proj\", m, i, o)))\n",
    "    hf_model.model.layers[0].mlp.down_proj.register_forward_hook(lambda m, i, o: hf_internal_outputs.update(hook_fn(\"mlp.down_proj\", m, i, o)))\n",
    "\n",
    "#  HookedTransformer \n",
    "def register_tl_hooks(model):\n",
    "    model.blocks[0].hook_resid_pre.register_forward_hook(lambda m, i, o: tl_internal_outputs.update(hook_fn(\"hook_resid_pre\", m, i, o)))\n",
    "    model.blocks[0].attn.hook_q.register_forward_hook(lambda m, i, o: tl_internal_outputs.update(hook_fn(\"hook_attn_in\", m, i, o)))\n",
    "    model.blocks[0].attn.hook_z.register_forward_hook(lambda m, i, o: tl_internal_outputs.update(hook_fn(\"hook_attn_out\", m, i, o)))\n",
    "    model.blocks[0].mlp.hook_pre.register_forward_hook(lambda m, i, o: tl_internal_outputs.update(hook_fn(\"hook_mlp_in\", m, i, o)))\n",
    "    model.blocks[0].mlp.hook_post.register_forward_hook(lambda m, i, o: tl_internal_outputs.update(hook_fn(\"hook_mlp_out\", m, i, o)))\n",
    "\n",
    "#  Hugging Face \n",
    "register_hf_hooks(hf_model)\n",
    "\n",
    "#  HookedTransformer \n",
    "register_tl_hooks(model)\n",
    "#  prompt\n",
    "prompt = \"What is the capital of France?\"\n",
    "\n",
    "#  tokenizer  prompt \n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "#  HookedTransformer  Hugging Face \n",
    "prompt_id_tl = tokenizer.encode(prompt, return_tensors=\"pt\").to(model_device)  # HookedTransformer \n",
    "prompt_id_hf = tokenizer.encode(prompt, return_tensors=\"pt\").to(hf_model_device)  # Hugging Face \n",
    "# \n",
    "tl_logits = model(prompt_id_tl).detach().cpu()\n",
    "hf_logits = hf_model(prompt_id_hf).logits.detach().cpu()\n",
    "\n",
    "# \n",
    "for tl_key, hf_key in module_mapping.items():\n",
    "    if tl_key in tl_internal_outputs and hf_key in hf_internal_outputs:\n",
    "        tl_value = tl_internal_outputs[tl_key]\n",
    "        hf_value = hf_internal_outputs[hf_key]\n",
    "        if not torch.allclose(tl_value, hf_value, atol=1e-4, rtol=1e-2):\n",
    "            print(f\"Difference found in {tl_key} (TL) vs {hf_key} (HF):\")\n",
    "            print(f\"HookedTransformer output: {tl_value}\")\n",
    "            print(f\"Hugging Face output: {hf_value}\")\n",
    "            print(f\"Difference: {tl_value - hf_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cuda:1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "hook_fn() missing 1 required positional argument: 'output'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[151], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m prompt_id_hf \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(prompts[\u001b[38;5;241m0\u001b[39m], return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Hugging Face\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#  HookedTransformer  CPU\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m tl_logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_id_tl\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#  Hugging Face  CPU\u001b[39;00m\n\u001b[1;32m     15\u001b[0m hf_logits \u001b[38;5;241m=\u001b[39m hf_model(prompt_id_hf)\u001b[38;5;241m.\u001b[39mlogits\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\n",
      "File \u001b[0;32m/aifs4su/yaodong/miniconda3/envs/sae/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/aifs4su/yaodong/miniconda3/envs/sae/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/aifs4su/yaodong/changye/TransformerLens/transformer_lens/HookedLlava.py:566\u001b[0m, in \u001b[0;36mHookedLlava.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, attention_mask, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    562\u001b[0m         shortformer_pos_embed \u001b[38;5;241m=\u001b[39m shortformer_pos_embed\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m    563\u001b[0m             devices\u001b[38;5;241m.\u001b[39mget_device_for_block_index(i, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg)\n\u001b[1;32m    564\u001b[0m         )\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;66;03m# import pdb; pdb.set_trace()\u001b[39;00m\n\u001b[0;32m--> 566\u001b[0m     residual \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each\u001b[39;49;00m\n\u001b[1;32m    569\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# block\u001b[39;49;00m\n\u001b[1;32m    570\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stop_at_layer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;66;03m# When we stop at an early layer, we end here rather than doing further computation\u001b[39;00m\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m residual\n",
      "File \u001b[0;32m/aifs4su/yaodong/miniconda3/envs/sae/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/aifs4su/yaodong/miniconda3/envs/sae/lib/python3.10/site-packages/torch/nn/modules/module.py:1568\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1565\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1566\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1568\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1569\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1570\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1571\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1572\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1573\u001b[0m     ):\n\u001b[1;32m   1574\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m/aifs4su/yaodong/changye/TransformerLens/transformer_lens/components/transformer_block.py:124\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry, attention_mask)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    108\u001b[0m     resid_pre: Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch pos d_model\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    111\u001b[0m     attention_mask: Optional[Int[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch offset_pos\u001b[39m\u001b[38;5;124m\"\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    112\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch pos d_model\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    113\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"A single Transformer block.\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \n\u001b[1;32m    115\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;124;03m        Float[torch.Tensor, \"batch pos d_model\"]: Our resulting tensor\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m     resid_pre \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhook_resid_pre\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresid_pre\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39muse_attn_in \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39muse_split_qkv_input:\n\u001b[1;32m    127\u001b[0m         \u001b[38;5;66;03m# We're adding a head dimension\u001b[39;00m\n\u001b[1;32m    128\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m shortformer_pos_embed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/aifs4su/yaodong/miniconda3/envs/sae/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/aifs4su/yaodong/miniconda3/envs/sae/lib/python3.10/site-packages/torch/nn/modules/module.py:1581\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1579\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, args, kwargs, result)\n\u001b[1;32m   1580\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1581\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1584\u001b[0m     result \u001b[38;5;241m=\u001b[39m hook_result\n",
      "Cell \u001b[0;32mIn[140], line 30\u001b[0m, in \u001b[0;36mregister_tl_hooks.<locals>.<lambda>\u001b[0;34m(m, i, o)\u001b[0m\n\u001b[1;32m     28\u001b[0m model\u001b[38;5;241m.\u001b[39mblocks[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mattn\u001b[38;5;241m.\u001b[39mhook_v\u001b[38;5;241m.\u001b[39mregister_forward_hook(\u001b[38;5;28;01mlambda\u001b[39;00m m, i, o: tl_internal_outputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattn.hook_v\u001b[39m\u001b[38;5;124m\"\u001b[39m: hook_fn(m, i, o)}))\n\u001b[1;32m     29\u001b[0m model\u001b[38;5;241m.\u001b[39mblocks[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mattn\u001b[38;5;241m.\u001b[39mhook_z\u001b[38;5;241m.\u001b[39mregister_forward_hook(\u001b[38;5;28;01mlambda\u001b[39;00m m, i, o: tl_internal_outputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattn.hook_z\u001b[39m\u001b[38;5;124m\"\u001b[39m: hook_fn(m, i, o)}))\n\u001b[0;32m---> 30\u001b[0m model\u001b[38;5;241m.\u001b[39mblocks[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mhook_resid_pre\u001b[38;5;241m.\u001b[39mregister_forward_hook(\u001b[38;5;28;01mlambda\u001b[39;00m m, i, o: tl_internal_outputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhook_resid_pre\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mhook_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m}))\n\u001b[1;32m     31\u001b[0m model\u001b[38;5;241m.\u001b[39mblocks[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mhook_resid_post\u001b[38;5;241m.\u001b[39mregister_forward_hook(\u001b[38;5;28;01mlambda\u001b[39;00m m, i, o: tl_internal_outputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhook_resid_post\u001b[39m\u001b[38;5;124m\"\u001b[39m: hook_fn(m, i, o)}))\n",
      "\u001b[0;31mTypeError\u001b[0m: hook_fn() missing 1 required positional argument: 'output'"
     ]
    }
   ],
   "source": [
    "\n",
    "#  prompt\n",
    "for i, prompt in enumerate(prompts):\n",
    "    print(f\"Processing prompt {i+1}/{len(prompts)}\")\n",
    "\n",
    "    # \n",
    "    prompt_id = tokenizer.encode(prompt, return_tensors=\"pt\").to(model_device)\n",
    "    prompt_id_hf = tokenizer.encode(prompt, return_tensors=\"pt\").to(hf_model_device)\n",
    "\n",
    "    #  0 \n",
    "    tl_input = \n",
    "    hf_input = prompt_id_hf\n",
    "    \n",
    "    #  0 \n",
    "    tl_layer_output = model.blocks[0](tl_input)\n",
    "    hf_layer_output = hf_model.model.layers[0](hf_input)\n",
    "\n",
    "    #  0 \n",
    "    if not torch.allclose(hf_layer_output, tl_layer_output, atol=1e-4, rtol=1e-2):\n",
    "        print(f\"Difference found at layer 0 for prompt {i}:\")\n",
    "        print(f\"hf_layer_output: {hf_layer_output}\")\n",
    "        print(f\"tl_layer_output: {tl_layer_output}\")\n",
    "        print(f\"Difference: {hf_layer_output - tl_layer_output}\")\n",
    "        \n",
    "        # \n",
    "        abs_diff = torch.max(torch.abs(hf_layer_output - tl_layer_output))\n",
    "        rel_diff = torch.max(torch.abs((hf_layer_output - tl_layer_output) / (tl_layer_output + 1e-8)))\n",
    "        print(f\"Max absolute difference at layer 0: {abs_diff.item()}\")\n",
    "        print(f\"Max relative difference at layer 0: {rel_diff.item()}\")\n",
    "\n",
    "        # \n",
    "        if not torch.allclose(hf_layer_output, tl_layer_output, atol=1e-3, rtol=1e-2):\n",
    "            print(f\"Larger difference persists at layer 0 for prompt {i}, investigate further.\")\n",
    "\n",
    "    # \n",
    "    assert torch.allclose(hf_layer_output, tl_layer_output, atol=1e-4, rtol=1e-2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"<image> Tell me what is in the image?\"\n",
    "# image_path = \"/aifs4su/yaodong/datasets/aaa_dataset/T2I-preference/0812_t2i_preference_dataset/image/285ed1502f68ad9737af9b4c059d76b82984421692a22f08eff793a0cc6301e3/36ad46bb0f79b5eedaed2818b2744a19d5f89e52e410b43140a875daabebf088.png\"\n",
    "# torch.set_printoptions(threshold=torch.inf)\n",
    "# from PIL import Image\n",
    "\n",
    "# image = Image.open(image_path)\n",
    "# input = processor(prompt, image, return_tensors=\"pt\").to(\n",
    "#             hf_model.device, dtype=hf_model.dtype\n",
    "#         )\n",
    "# input_ids = input.input_ids\n",
    "# print(input_ids)\n",
    "input_ids = torch.tensor([[    0,  8197, \n",
    "                          3643,   285,  5867,   453,  6540,  6488,   376,  7347,  6468,  1656,\n",
    "         1057,  1910,  6482,  4008,  5611,   376,  3706,  2941,  7444,  7444,\n",
    "          712,  5664,  4476,  3234,  6307,   798,  7444,  6345,  5589,  3158,\n",
    "           40,  1323,   376,  3643,  3181,  1641,   448,  3743,  2895,  1971,\n",
    "          376,  3642,  5439,  6097,  6820,   226,  3666,   750,  5867,   269,\n",
    "         7444,  1343,  4214,  2229,  1093,  3000,  6009,  3214,  7755,  4599,\n",
    "         6420,  5409,  7873,  3396,  6614,   827,  3872,  1741,  7536,   581,\n",
    "         7349,  4187,  7444,  7444,  3706,   376,  4317,  1303,  7518,  1062,\n",
    "         1846,  3214,  6488,  4746,  2419,  4462,  6035,  6498,  5104,  7634,\n",
    "         4129,  2991,  3356,   182,  2872,  4187,  7903,  4237,  2095,  2873,\n",
    "         4359,  2128,  4405,  1527,  2895,  3772,   798,  1385,  3643,   362,\n",
    "         7065,  6305,  4102,   376,  5906,  1910,   805,  4502,  2868,  3048,\n",
    "         8015,  2217,  1596,  2029,  7303,  6045,  7626,  1930,  4472,   251,\n",
    "         4591,  6811,  7162,  6684,  3048,  2359,  2394,  3214,  4134,  2941,\n",
    "         7783,  1846,   610,  2953,   863,   805,  6306,    37,  6113,  1238,\n",
    "         6486,  7991,  7729,  1689,  2290,   651,  6891,  5312,  2644,  5291,\n",
    "         4384,  4274,  3195,  3296,  6435,  4983,   589,  1846,   481,    10,\n",
    "         4707,  3320,  6529,   587,  1578,  2659,  6248,  6482,  3214,  4503,\n",
    "         2881,  5021,   350,  5054,  4234,   685,  6805,  1424,    41,  5274,\n",
    "         3911,   453,  4084,  5711,    56,  7528,  7720,  5813,  5918,  2348,\n",
    "          792,  1787,  3025,  6037,  3706,  6820,   805,  3412,  7444,  7893,\n",
    "         6291,  2547,  1846,  3214,  1324,  6078,  5267,   249,  1240,  6435,\n",
    "         6761,  5366,  4488,   777,   238,  4554,  4436,  1541,  4187,  1930,\n",
    "         5949,  3983,  1028,  4661,  5054,  4503,  3911,  4659,  3144,   481,\n",
    "          792,  3144,  5592,   249,  7395,    77,   363,  4309,  1067,  1590,\n",
    "         2202,  3643,  2001,  2982,  7201,   604,  4666,  2325,  3110,    63,\n",
    "         4400,    10,  5730,  6778,   219,  4149,  7905,  5432,  2658,  4642,\n",
    "         8044,  4790,  4440,   312,  1414,   625,  2182,   363,  2225,  4649,\n",
    "         6874,  3163,  4791,  5441,  7229,   362,  2466,   191,   481,  4498,\n",
    "         6922,  1324,  3605,  1272,   863,  2805,  2561,  7189,  4593,  5192,\n",
    "         3859,  4180,  3218,  3188,  4149,  3252,  3437,  2317,  2820,    10,\n",
    "         7749,  7569,   675,  1847,  3772,  3427,  3584,  7437,  5447,  3083,\n",
    "         3248,   604,  4129,  5488,  1245,  2001,  6906,  5936,  6718,  1351,\n",
    "         1022,  6750,  3609,  3549,   177,  1526,  7893,  7380,  1306,  2557,\n",
    "         1237,  1218,  1106,  4276,  3610,  6358,  2132,  4440,  5137,   742,\n",
    "         4869,  5296,   363,  5137,  3204,  7036,  4077,  3819,  2269,  3197,\n",
    "         6512,  1748,  1292,  4077,  4681,  1091,  7647,  3819,  7489,  2886,\n",
    "         6380,  4659,  2001,  4284,  5680,  3020,  6622,   930,  5124,  4489,\n",
    "         1876,  3000,  3819,  2454,  3248,   363,   363,  5519,  2992,  6775,\n",
    "         7334,  2132,  6068,  1022,  3221,  4077,  2347,   363,  1754,  4422,\n",
    "         7647,  2886,  2132,  3452,  6725,  4676,  7102,  1028,  4086,  2066,\n",
    "         4128,  7031,  2501,  4642,  6291,  5866,  1858,  3963,  2132,  3437,\n",
    "          302,  4818,   445,  1106,  4529,  1681,  1641,  5540,  2001,   363,\n",
    "         7489,  3605,  7647,  2132,  4354,  1930,  7402,  5192,   953,  5946,\n",
    "         7524,  3810,  6072,  3181,  3020,  5486,  3382,  2672,   407,  3810,\n",
    "         4671,  6522,  6782,  7547,  6115,  2132,  5716,  5375,  7450,  3661,\n",
    "         4211,  1106,  2590,  1861,  3601,  3356,  1263,  3540,  1208,  6360,\n",
    "         3900,  5278,   862,  6607,   103,  4537,  4446,  3206,  5239,  2908,\n",
    "         6584,  1858,  2284,  3319,  5503,  4135,  4671,  5307,  7631,   354,\n",
    "         5293,  4374,   221,  2749,  6861,  4211,  3665,  4029,  5307,  1682,\n",
    "         3188,  1245,    19,   502,   390,  5798,  4239,  3020,  4686,  4184,\n",
    "         4184,  2243,  3831,  3831,  5706,  6260,  2240,  3135,  4259,  2064,\n",
    "         3168,  8081,  3020,  1974,  1016,  4180,  4146,  6085,   637,  1331,\n",
    "         1079,   741,  6725,  3862,  3190,  1745,  5779,  1453,  5402,  4821,\n",
    "         2036,  5866,  1578,  1298,  1706,  3631,   390,  5861,  1708,  1938,\n",
    "         5034,  1049,  6524,  1876,  1648,  4574,  5074,  8044,  4317,  1882,\n",
    "          975,  2900,  4251,  2900,  1420,   133,  1453,  1077,  2784,  1332,\n",
    "         4837,  2773,  3244,  7514,  4416,   894,   346,  5483,  6345,  7622,\n",
    "         5486,  1642,  2787,  5378,   973,  7099,   614,  7748,  4953,   777,\n",
    "         5127,  5757,  1558,  4729,  7292,  1331,  3420,  2401,  2731,  1298,\n",
    "          943,  6629,  5838,  5905,  7935,  4163,  6199,   551,  4968,  4951,\n",
    "          141,  3181,  3155,   480,  4512,  3544,  2318,  2145,  4213,  6852,\n",
    "         8147,  4300,  3446,  6511,  3891,  7374,  7774,  5906,  1659,  5592,\n",
    "         3096,  4192,   229,  3242,  4477,  3688,  4298,  5618,  2445,  1628,\n",
    "         1557,  3841,   833,   381,  7065,  3762,  3172,  3020,   689,  4146,\n",
    "         1838,  7729,  7287,   768,  2291,  3008,  3543,  7864,  6028,   959,\n",
    "         7603,  2508,   142,  7528,  4254,  6896,  7075,  2941,  1264,  1582,\n",
    "         1930,  2048,   543,  6139,  2961,  4405,  2289,  2004,  5865,  8030,\n",
    "         6524,  3008,  6438,  5278,  4266,  2291,  4407,  1181,  3296,  3110,\n",
    "         5652,  5320,  6390,  2659,  1018,  4192,  7592,  5723,  5564,  1237,\n",
    "         2268,  3296,  4029,  1057,   520,  2463,  4883,    79,  2514,  6390,\n",
    "          693,  1611,  3610,  3946,  1832,  6800,  4478,   177,  7990,  2941,\n",
    "         7154,  2895,  4877,  3606,  1522,   854,  3172,  6290,  3559,   905,\n",
    "         4405,  1630,  7347,  3778,  4029,  4484,  5453,  6191,  7183,  3769,\n",
    "         7510,  4686,  6629,  4615,  3042,  5681,  1440,   630,  7011,  5342,\n",
    "         2380,  3645,   467,  3155,  5774,  1166,  1459,  7347,  5103,   249,\n",
    "          346,  5664,  6291,  8044,  8067,  4986,  4462,  2245,  5578,  1215,\n",
    "         3108,  4331,  5271,  1645,  1202,  1288,  5554,  1745,  1331,  7032,\n",
    "         1028,  1067,  6462,  1630,   947,  6155,  3016,  5278,   242,  5785,\n",
    "         2869,  2744,  2744,  7626,  8187,  1503,  3636,  2716,  4485,  7047,\n",
    "         2443,  2372,  3008,  1846,  1363,  2036,  3108,   312,  5428,  4976,\n",
    "           46,  2466,  3255,  1930,  5892,  4503,  5320,  2113,  1584,  2338,\n",
    "          318,   476,  2998,  3907,  1747,   742,  2492,  3808,  5525,   386,\n",
    "         5362,  4849,   930,  3767,  1213,  1067,  3103,  5026,  8160,  4821,\n",
    "         2126,  5313,  4239,  2287,  3252,  3214,  1414,  1526,  2645,  6111,\n",
    "         1317,  5478,   382,  4780,  2012,  6664,  5682,  3769,  1832,  4530,\n",
    "         4512,   275,   714,   474,  2021,  6706,  2291,   355,  4276,  5376,\n",
    "         7195,  5901,   610,  2268,  2941,  1223,  4574,  2287,  4008,  1810,\n",
    "         1420,  7078,  2765,   635,  3867,  5662,    79,  8140,  7820,  7510,\n",
    "         3181,  2998,  3220,   318,  5255,  1306,  6922,  7380,  1531,   429,\n",
    "         3221,   543,  6761,   476,  2500,  3245,  6571,  6199,  2387,  3096,\n",
    "         6814,  1354,   348,  6162,  4596,  1440,  2036,    37,  2460,  4590,\n",
    "         4686,  1363,  3447,   472,  6199,  3373,  7487,  6486,  3645,  1199,\n",
    "         5066,  4970,  2287,  3412,  1240,  4955,  3131,  1201,  6271,  7198,\n",
    "          269,  4951,  3373,  6953,  7351,  5871,  4321,  5711,   881,  6861,\n",
    "          486,  7192,  5103,  2289,  1198,  7394,  3459,  6045,  4550,  3345,\n",
    "          931,  3868,  5146,  4837,  4780,   703,  2965,  1157,  4676,  6203,\n",
    "         1610,  6941,   182,  7287,  4819,   226,  2869,  3686,  6024,  4145,\n",
    "         7489,  1223,  6486,  3614,   312,   550,  5259,  2174,  1198,  6723,\n",
    "          735,  2557,  5946,  1068,  6177,   974,  3898,  2788,  1610,  4409,\n",
    "         7834,   789,  7098,  6198,  8060,  6198,   295,   224,    67,  2561,\n",
    "         5737,  4407,  2384,   651,  7278,  4135,  2153,  5408,  3129,  7705,\n",
    "          476,  6248,  3033,  3706,  8196,  28862, 16848, 17016,\n",
    "         16704, 16672, 16647, 19521, 16414,  8710]])\n",
    "\n",
    "print(input_ids.shape)\n",
    "output = model.generate(input_ids, max_new_tokens=10, do_sample=False)\n",
    "print(output[0])\n",
    "print(processor.tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading LLAVA from transformers\n",
    "\n",
    "Load a chameleon model from transformers, and compare the outputs, the logits, and the hidden states to ensure we did a good job integrating the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model = hf_model.to(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Where is the capital of Germany?\"\n",
    "# image_path = \"/aifs4su/yaodong/datasets/aaa_dataset/T2I-preference/0812_t2i_preference_dataset/image/285ed1502f68ad9737af9b4c059d76b82984421692a22f08eff793a0cc6301e3/36ad46bb0f79b5eedaed2818b2744a19d5f89e52e410b43140a875daabebf088.png\"\n",
    "\n",
    "# from PIL import Image\n",
    "\n",
    "# image = Image.open(image_path)\n",
    "input = processor(prompt, return_tensors=\"pt\").to(\n",
    "            hf_model.device, dtype=hf_model.dtype\n",
    "        )\n",
    "print(input.input_ids)\n",
    "input_ids = input.input_ids\n",
    "\n",
    "output = hf_model.generate(input_ids.to(hf_model.device), max_new_tokens=20, do_sample=False)\n",
    "print(processor.tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get shape of the weights\n",
    "# for name in hf_model.state_dict():\n",
    "#     print(name, hf_model.state_dict()[name].shape)\n",
    "    \n",
    "# print(hf_model.state_dict()[\"model.layers.0.self_attn.q_norm.weight\"])\n",
    "# print(hf_model.state_dict()[\"model.layers.0.input_layernorm.weight\"])\n",
    "# print(hf_model.state_dict()[\"model.layers.0.post_attention_layernorm.weight\"])\n",
    "# print(hf_model.state_dict()[\"model.layers.0.self_attn.q_norm.weight\"])\n",
    "# print(hf_model.state_dict()[\"model.layers.0.self_attn.q_proj.weight\"])\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block 0 is: ('', MistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32064, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32064, bias=False)\n",
      "))\n",
      "Block 1 is: ('model', MistralModel(\n",
      "  (embed_tokens): Embedding(32064, 4096)\n",
      "  (layers): ModuleList(\n",
      "    (0-31): 32 x MistralDecoderLayer(\n",
      "      (self_attn): MistralAttention(\n",
      "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "        (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (rotary_emb): MistralRotaryEmbedding()\n",
      "      )\n",
      "      (mlp): MistralMLP(\n",
      "        (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "        (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "        (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "        (act_fn): SiLU()\n",
      "      )\n",
      "      (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "      (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "    )\n",
      "  )\n",
      "  (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "))\n",
      "Block 2 is: ('model.embed_tokens', Embedding(32064, 4096))\n",
      "Block 3 is: ('model.layers', ModuleList(\n",
      "  (0-31): 32 x MistralDecoderLayer(\n",
      "    (self_attn): MistralAttention(\n",
      "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      (rotary_emb): MistralRotaryEmbedding()\n",
      "    )\n",
      "    (mlp): MistralMLP(\n",
      "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "    (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      "))\n",
      "Block 4 is: ('model.layers.0', MistralDecoderLayer(\n",
      "  (self_attn): MistralAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): MistralMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "))\n",
      "Block 5 is: ('model.layers.0.self_attn', MistralAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): MistralRotaryEmbedding()\n",
      "))\n",
      "Block 6 is: ('model.layers.0.self_attn.q_proj', Linear(in_features=4096, out_features=4096, bias=False))\n",
      "Block 7 is: ('model.layers.0.self_attn.k_proj', Linear(in_features=4096, out_features=1024, bias=False))\n",
      "Block 8 is: ('model.layers.0.self_attn.v_proj', Linear(in_features=4096, out_features=1024, bias=False))\n",
      "Block 9 is: ('model.layers.0.self_attn.o_proj', Linear(in_features=4096, out_features=4096, bias=False))\n",
      "Block 10 is: ('model.layers.0.self_attn.rotary_emb', MistralRotaryEmbedding())\n",
      "Block 11 is: ('model.layers.0.mlp', MistralMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      "))\n",
      "Block 12 is: ('model.layers.0.mlp.gate_proj', Linear(in_features=4096, out_features=14336, bias=False))\n",
      "Block 13 is: ('model.layers.0.mlp.up_proj', Linear(in_features=4096, out_features=14336, bias=False))\n",
      "Block 14 is: ('model.layers.0.mlp.down_proj', Linear(in_features=14336, out_features=4096, bias=False))\n",
      "Block 15 is: ('model.layers.0.mlp.act_fn', SiLU())\n",
      "Block 16 is: ('model.layers.0.input_layernorm', MistralRMSNorm((4096,), eps=1e-05))\n",
      "Block 17 is: ('model.layers.0.post_attention_layernorm', MistralRMSNorm((4096,), eps=1e-05))\n",
      "Block 18 is: ('model.layers.1', MistralDecoderLayer(\n",
      "  (self_attn): MistralAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (rotary_emb): MistralRotaryEmbedding()\n",
      "  )\n",
      "  (mlp): MistralMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
      "))\n",
      "Block 19 is: ('model.layers.1.self_attn', MistralAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (rotary_emb): MistralRotaryEmbedding()\n",
      "))\n",
      "Block 20 is: ('model.layers.1.self_attn.q_proj', Linear(in_features=4096, out_features=4096, bias=False))\n",
      "Block 21 is: ('model.layers.1.self_attn.k_proj', Linear(in_features=4096, out_features=1024, bias=False))\n",
      "Block 22 is: ('model.layers.1.self_attn.v_proj', Linear(in_features=4096, out_features=1024, bias=False))\n",
      "Block 23 is: ('model.layers.1.self_attn.o_proj', Linear(in_features=4096, out_features=4096, bias=False))\n",
      "Block 24 is: ('model.layers.1.self_attn.rotary_emb', MistralRotaryEmbedding())\n",
      "Block 25 is: ('model.layers.1.mlp', MistralMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      "))\n",
      "Block 26 is: ('model.layers.1.mlp.gate_proj', Linear(in_features=4096, out_features=14336, bias=False))\n",
      "Block 27 is: ('model.layers.1.mlp.up_proj', Linear(in_features=4096, out_features=14336, bias=False))\n",
      "Block 28 is: ('model.layers.1.mlp.down_proj', Linear(in_features=14336, out_features=4096, bias=False))\n",
      "Block 29 is: ('model.layers.1.mlp.act_fn', SiLU())\n",
      "Block 30 is: ('model.layers.1.input_layernorm', MistralRMSNorm((4096,), eps=1e-05))\n",
      "Block 31 is: ('model.layers.1.post_attention_layernorm', MistralRMSNorm((4096,), eps=1e-05))\n"
     ]
    }
   ],
   "source": [
    "hf_blocks_and_idxs = list(zip(range(hf_model.config.num_hidden_layers), hf_model.named_modules()))\n",
    "for i, block in hf_blocks_and_idxs:\n",
    "    print(f\"Block {i} is: {block}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare logits with HuggingFace model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:35<?, ?it/s]\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'extreme_negative_value' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mtokenizer\n\u001b[1;32m     11\u001b[0m prompt_ids \u001b[38;5;241m=\u001b[39m [tokenizer\u001b[38;5;241m.\u001b[39mencode(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m---> 12\u001b[0m tl_logits \u001b[38;5;241m=\u001b[39m [model(prompt_ids)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu() \u001b[38;5;28;01mfor\u001b[39;00m prompt_ids \u001b[38;5;129;01min\u001b[39;00m tqdm(prompt_ids)]\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# hf logits are really slow as it's on CPU. If you have a big/multi-GPU machine, run `hf_model = hf_model.to(\"cuda\")` to speed this up\u001b[39;00m\n\u001b[1;32m     15\u001b[0m logits \u001b[38;5;241m=\u001b[39m [hf_model(prompt_ids\u001b[38;5;241m.\u001b[39mto(hf_model\u001b[38;5;241m.\u001b[39mdevice))\u001b[38;5;241m.\u001b[39mlogits\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu() \u001b[38;5;28;01mfor\u001b[39;00m prompt_ids \u001b[38;5;129;01min\u001b[39;00m tqdm(prompt_ids)]\n",
      "Cell \u001b[0;32mIn[73], line 12\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mtokenizer\n\u001b[1;32m     11\u001b[0m prompt_ids \u001b[38;5;241m=\u001b[39m [tokenizer\u001b[38;5;241m.\u001b[39mencode(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m---> 12\u001b[0m tl_logits \u001b[38;5;241m=\u001b[39m [\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_ids\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu() \u001b[38;5;28;01mfor\u001b[39;00m prompt_ids \u001b[38;5;129;01min\u001b[39;00m tqdm(prompt_ids)]\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# hf logits are really slow as it's on CPU. If you have a big/multi-GPU machine, run `hf_model = hf_model.to(\"cuda\")` to speed this up\u001b[39;00m\n\u001b[1;32m     15\u001b[0m logits \u001b[38;5;241m=\u001b[39m [hf_model(prompt_ids\u001b[38;5;241m.\u001b[39mto(hf_model\u001b[38;5;241m.\u001b[39mdevice))\u001b[38;5;241m.\u001b[39mlogits\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu() \u001b[38;5;28;01mfor\u001b[39;00m prompt_ids \u001b[38;5;129;01min\u001b[39;00m tqdm(prompt_ids)]\n",
      "File \u001b[0;32m/aifs4su/yaodong/miniconda3/envs/sae/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/aifs4su/yaodong/miniconda3/envs/sae/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/aifs4su/yaodong/changye/TransformerLens/transformer_lens/HookedLlava.py:566\u001b[0m, in \u001b[0;36mHookedLlava.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, attention_mask, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    562\u001b[0m         shortformer_pos_embed \u001b[38;5;241m=\u001b[39m shortformer_pos_embed\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m    563\u001b[0m             devices\u001b[38;5;241m.\u001b[39mget_device_for_block_index(i, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg)\n\u001b[1;32m    564\u001b[0m         )\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;66;03m# import pdb; pdb.set_trace()\u001b[39;00m\n\u001b[0;32m--> 566\u001b[0m     residual \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each\u001b[39;49;00m\n\u001b[1;32m    569\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# block\u001b[39;49;00m\n\u001b[1;32m    570\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpast_kv_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stop_at_layer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;66;03m# When we stop at an early layer, we end here rather than doing further computation\u001b[39;00m\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m residual\n",
      "File \u001b[0;32m/aifs4su/yaodong/miniconda3/envs/sae/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/aifs4su/yaodong/miniconda3/envs/sae/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/aifs4su/yaodong/changye/TransformerLens/transformer_lens/components/transformer_block.py:164\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry, attention_mask)\u001b[0m\n\u001b[1;32m    158\u001b[0m     value_input \u001b[38;5;241m=\u001b[39m attn_in\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# import pdb; pdb.set_trace()\u001b[39;00m\n\u001b[1;32m    160\u001b[0m attn_out \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# hook the residual stream states that are used to calculate the\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# queries, keys and values, independently.\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# Then take the layer norm of these inputs, and pass these to the attention module.\u001b[39;00m\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mshortformer_pos_embed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_kv_cache_entry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m )  \u001b[38;5;66;03m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39muse_normalization_before_and_after:\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;66;03m# If we use LayerNorm both before and after, then apply the second LN after the layer\u001b[39;00m\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;66;03m# and before the hook. We do it before the hook so hook_attn_out captures \"that which\u001b[39;00m\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;66;03m# is added to the residual stream\"\u001b[39;00m\n\u001b[1;32m    178\u001b[0m     attn_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1_post(attn_out)\n",
      "File \u001b[0;32m/aifs4su/yaodong/miniconda3/envs/sae/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/aifs4su/yaodong/miniconda3/envs/sae/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/aifs4su/yaodong/changye/TransformerLens/transformer_lens/components/Mistral_attention.py:215\u001b[0m, in \u001b[0;36mMistralAttention.forward\u001b[0;34m(self, query_input, key_input, value_input, past_kv_cache_entry, additive_attention_mask, attention_mask, position_bias)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# if self.cfg.positional_embedding_type == \"alibi\":\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m#     query_ctx = attn_scores.size(-2)\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m#     # The key context length is the number of positions in the past - this includes all positions in the cache\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    211\u001b[0m \n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m#     attn_scores += position_bias\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mattention_dir \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcausal\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;66;03m# If causal attention, we mask it to only attend backwards. If bidirectional, we don't mask.\u001b[39;00m\n\u001b[0;32m--> 215\u001b[0m     attn_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_causal_mask\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache_pos_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, head_index, query_pos, key_pos]\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m additive_attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    219\u001b[0m     attn_scores \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m additive_attention_mask\n",
      "File \u001b[0;32m/aifs4su/yaodong/changye/TransformerLens/transformer_lens/components/Mistral_attention.py:125\u001b[0m, in \u001b[0;36mMistralAttention.apply_causal_mask\u001b[0;34m(self, attn_scores, past_kv_pos_offset, attention_mask)\u001b[0m\n\u001b[1;32m    123\u001b[0m     extreme_negative_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3.4028e+38\u001b[39m, device\u001b[38;5;241m=\u001b[39mattention_mask\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    124\u001b[0m attn_scores \u001b[38;5;241m=\u001b[39m attn_scores\u001b[38;5;241m.\u001b[39mto(final_mask\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mwhere(final_mask, attn_scores, \u001b[43mextreme_negative_value\u001b[49m)\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'extreme_negative_value' referenced before assignment"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"Where is the capital of Germany?\",\n",
    "    \"Calculate 2 * 42 = \", \n",
    "    \"My favorite\", \n",
    "    \"My favorite place is\",\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "hf_model.eval()\n",
    "tokenizer = processor.tokenizer\n",
    "prompt_ids = [tokenizer.encode(prompt, return_tensors=\"pt\") for prompt in prompts]\n",
    "tl_logits = [model(prompt_ids).detach().cpu() for prompt_ids in tqdm(prompt_ids)]\n",
    "\n",
    "# hf logits are really slow as it's on CPU. If you have a big/multi-GPU machine, run `hf_model = hf_model.to(\"cuda\")` to speed this up\n",
    "logits = [hf_model(prompt_ids.to(hf_model.device)).logits.detach().cpu() for prompt_ids in tqdm(prompt_ids)]\n",
    "\n",
    "for i in range(len(prompts)): \n",
    "    if not torch.allclose(logits[i], tl_logits[i], atol=1e-2, rtol=1e-2):\n",
    "        print(f\"Logits for prompt {i} are not close\")\n",
    "        print(f\"Logits from HuggingFace: shape {logits[i].shape}\")\n",
    "        print(f\"Logits from TransformerLens: shape {tl_logits[i].shape}\")\n",
    "        diff = torch.abs(logits[i] - tl_logits[i]) > 1e-2\n",
    "        indices = torch.nonzero(diff)\n",
    "        for index in indices:\n",
    "            row, col, loc = index[0], index[1], index[2]\n",
    "            print(f\"Diff at {index}: HuggingFace={logits[i][row, col, loc]}, TransformerLens={tl_logits[i][row, col, loc]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare hidden states\n",
    "\n",
    "tl_hidden_states = [model(prompt_ids, return_type=\"hidden_states\", stop_at_layer=1).detach().cpu() for prompt_ids in tqdm(prompt_ids)]\n",
    "hf_hidden_states = [hf_model(prompt_ids.to(hf_model.device), output_hidden_states=True, output_attentions=True).hidden_states[1].detach().cpu() for prompt_ids in tqdm(prompt_ids)]\n",
    "\n",
    "for i in range(len(prompts)): \n",
    "    print(f\"Shape of hf hidden states: {hf_hidden_states[i].shape}\")\n",
    "    print(f\"Shape of tl hidden states: {tl_hidden_states[i].shape}\")\n",
    "    if not torch.allclose(hf_hidden_states[i], tl_hidden_states[i], atol=1e-4, rtol=1e-2):\n",
    "        print(f\"Hidden states for prompt {i} are not close\")\n",
    "    print(f\"Hidden states from HuggingFace: {hf_hidden_states[i]}\")\n",
    "    print(f\"Hidden states from TransformerLens: {tl_hidden_states[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare attentions\n",
    "\n",
    "tl_attentions = [model(prompt_ids, return_type=\"attentions\")[2].detach().cpu() for prompt_ids in tqdm(prompt_ids)]\n",
    "hf_attentions = [hf_model(prompt_ids.to(hf_model.device), output_hidden_states=True, output_attentions=True).attentions[2].detach().cpu() for prompt_ids in tqdm(prompt_ids)]\n",
    "\n",
    "for i in range(len(prompts)): \n",
    "    print(f\"Shape of hf attentions: {hf_attentions[i].shape}\")\n",
    "    print(f\"Shape of tl attentions: {tl_attentions[i].shape}\")\n",
    "    if not torch.allclose(hf_attentions[i], tl_attentions[i], atol=1e-4, rtol=1e-2):\n",
    "        print(f\"Attentions for prompt {i} are not close\")\n",
    "        print(f\"Attentions from HuggingFace: {hf_attentions[i]}\")\n",
    "        print(f\"Attentions from TransformerLens: {tl_attentions[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TransformerLens Demo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading from hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_text = \"Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets.\"\n",
    "llama_tokens = model.to_tokens(llama_text)\n",
    "llama_logits, llama_cache = model.run_with_cache(llama_tokens, remove_batch_dim=True)\n",
    "\n",
    "attention_pattern = llama_cache[\"pattern\", 0, \"attn\"]\n",
    "llama_str_tokens = model.to_str_tokens(llama_text)\n",
    "\n",
    "print(\"Layer 0 Head Attention Patterns:\")\n",
    "display(cv.attention.attention_patterns(tokens=llama_str_tokens, attention=attention_pattern))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing to hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_to_ablate = 0\n",
    "head_index_to_ablate = 31\n",
    "\n",
    "# We define a head ablation hook\n",
    "# The type annotations are NOT necessary, they're just a useful guide to the reader\n",
    "# \n",
    "def head_ablation_hook(\n",
    "    value: Float[torch.Tensor, \"batch pos head_index d_head\"],\n",
    "    hook: HookPoint\n",
    ") -> Float[torch.Tensor, \"batch pos head_index d_head\"]:\n",
    "    print(f\"Shape of the value tensor: {value.shape}\")\n",
    "    value[:, :, head_index_to_ablate, :] = 0.\n",
    "    return value\n",
    "\n",
    "original_loss = model(llama_tokens, return_type=\"loss\")\n",
    "ablated_loss = model.run_with_hooks(\n",
    "    llama_tokens, \n",
    "    return_type=\"loss\", \n",
    "    fwd_hooks=[(\n",
    "        utils.get_act_name(\"v\", layer_to_ablate), \n",
    "        head_ablation_hook\n",
    "        )]\n",
    "    )\n",
    "print(f\"Original Loss: {original_loss.item():.3f}\")\n",
    "print(f\"Ablated Loss: {ablated_loss.item():.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.10 ('tl-llama': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f03ec946e3b5caa7cc710a963f479e62a68fff56c790a7066e03c8b5c22adad9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
